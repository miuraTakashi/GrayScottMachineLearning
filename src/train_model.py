#!/usr/bin/env python3
"""
Gray-Scott 3D CNN Autoencoder Training Script
æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è‡ªå‹•æ¢ç´¢æ©Ÿèƒ½ä»˜ã
"""

import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from gray_scott_autoencoder import GrayScottDataset, Conv3DAutoencoder

def find_optimal_clusters_simple(latent_vectors, max_k=20):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªæœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°æ¢ç´¢ï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æãƒ™ãƒ¼ã‚¹ï¼‰
    """
    print("ğŸ” æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’æ¢ç´¢ä¸­...")
    
    silhouette_scores = []
    k_range = range(2, min(max_k + 1, len(latent_vectors) // 2))
    
    best_score = -1
    best_k = 6  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(latent_vectors)
        score = silhouette_score(latent_vectors, cluster_labels)
        silhouette_scores.append(score)
        
        if score > best_score:
            best_score = score
            best_k = k
        
        print(f"  k={k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {score:.3f}")
    
    print(f"ğŸ¯ æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: k={best_k} (ã‚¹ã‚³ã‚¢: {best_score:.3f})")
    
    return best_k, silhouette_scores, list(k_range)

def get_user_cluster_choice(recommended_k, silhouette_scores, k_range):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®é¸æŠã‚’æ±‚ã‚ã‚‹
    """
    print("\n" + "="*60)
    print("ğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®é¸æŠ")
    print("="*60)
    
    # ä¸Šä½3ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’è¡¨ç¤º
    score_k_pairs = list(zip(silhouette_scores, k_range))
    score_k_pairs.sort(reverse=True)  # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    
    print("ğŸ“Š ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ä¸Šä½:")
    for i, (score, k) in enumerate(score_k_pairs[:5]):
        marker = "ğŸ†" if k == recommended_k else f"{i+1}."
        print(f"  {marker} k={k}: {score:.3f}")
    
    print(f"\nğŸ’¡ æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {recommended_k}")
    print(f"ğŸ“ˆ åˆ©ç”¨å¯èƒ½ãªç¯„å›²: {min(k_range)}ï½{max(k_range)}")
    
    while True:
        try:
            print("\né¸æŠã—ã¦ãã ã•ã„:")
            print(f"1. æ¨å¥¨å€¤ã‚’ä½¿ç”¨ (k={recommended_k})")
            print("2. æ‰‹å‹•ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’æŒ‡å®š")
            print("3. ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º")
            
            choice = input("é¸æŠ (1/2/3): ").strip()
            
            if choice == "1":
                final_k = recommended_k
                print(f"âœ… æ¨å¥¨å€¤ k={final_k} ã‚’ä½¿ç”¨ã—ã¾ã™")
                break
            
            elif choice == "2":
                manual_k = int(input(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’å…¥åŠ› ({min(k_range)}ï½{max(k_range)}): "))
                if manual_k in k_range:
                    final_k = manual_k
                    # é¸æŠã—ãŸkã®ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                    selected_score = silhouette_scores[k_range.index(manual_k)]
                    print(f"âœ… k={final_k} ã‚’é¸æŠã—ã¾ã—ãŸ (ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {selected_score:.3f})")
                    break
                else:
                    print(f"âŒ {manual_k} ã¯ç¯„å›²å¤–ã§ã™ã€‚{min(k_range)}ï½{max(k_range)}ã®å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            
            elif choice == "3":
                # ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
                plt.figure(figsize=(10, 6))
                plt.plot(k_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)
                plt.axvline(x=recommended_k, color='red', linestyle='--', 
                           label=f'Recommended k = {recommended_k}')
                plt.xlabel('Number of Clusters (k)')
                plt.ylabel('Silhouette Score')
                plt.title('Silhouette Analysis for Optimal k')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                # å„ç‚¹ã«ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                for i, (k, score) in enumerate(zip(k_range, silhouette_scores)):
                    plt.annotate(f'{score:.3f}', (k, score), 
                               textcoords="offset points", xytext=(0,10), ha='center')
                
                plt.tight_layout()
                plt.show()
                print("ğŸ“Š ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚")
            
            else:
                print("âŒ 1, 2, ã¾ãŸã¯ 3 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        except ValueError:
            print("âŒ æœ‰åŠ¹ãªæ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print(f"\nâš ï¸  ä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚æ¨å¥¨å€¤ k={recommended_k} ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            final_k = recommended_k
            break
    
    return final_k

def main():
    print("ğŸ”¬ Gray-Scott 3D CNN Autoencoder Training")
    print("æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è‡ªå‹•æ¢ç´¢æ©Ÿèƒ½ä»˜ã")
    print("=" * 60)
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    gif_folder = 'gif'
    batch_size = 4
    fixed_frames = 30
    target_size = (64, 64)
    latent_dim = 64
    num_epochs = 50
    learning_rate = 0.001
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­...")
    dataset = GrayScottDataset(gif_folder, fixed_frames=fixed_frames, target_size=target_size)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
    
    # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’å–å¾—ï¼ˆæœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ï¼‰
    first_sample = dataset[0]
    sample_tensor = first_sample['tensor']
    print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {sample_tensor.shape}")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\nğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = Conv3DAutoencoder(latent_dim=latent_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()
    
    print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # å­¦ç¿’
    print(f"\nğŸš€ å­¦ç¿’é–‹å§‹ ({num_epochs} ã‚¨ãƒãƒƒã‚¯)...")
    model.train()
    losses = []
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        for batch_idx, batch in enumerate(dataloader):
            data = batch['tensor'].to(device)
            
            optimizer.zero_grad()
            reconstructed, _ = model(data)
            loss = criterion(reconstructed, data)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 20 == 0:
                print(f"  Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.4f}")
        
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        print(f"âœ… Epoch {epoch+1}/{num_epochs} å®Œäº†, å¹³å‡Loss: {avg_loss:.4f}")
    
    # å­¦ç¿’æ›²ç·šã‚’ä¿å­˜
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.grid(True)
    plt.savefig('training_loss.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç‰¹å¾´æŠ½å‡ºï¼ˆæ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
    print("\nğŸ” ç‰¹å¾´æŠ½å‡ºä¸­...")
    model.eval()
    all_latent_vectors = []
    all_filenames = []
    all_f_values = []
    all_k_values = []
    
    with torch.no_grad():
        for batch in dataloader:
            data = batch['tensor'].to(device)
            filenames = batch['filename']
            f_vals = batch['f_value']
            k_vals = batch['k_value']
            
            _, latent = model(data)
            all_latent_vectors.append(latent.cpu().numpy())
            all_filenames.extend(filenames)
            all_f_values.extend(f_vals)
            all_k_values.extend(k_vals)
    
    # é…åˆ—ã«å¤‰æ›
    latent_vectors = np.vstack(all_latent_vectors)
    f_values = np.array(all_f_values, dtype=np.float32)
    k_values = np.array(all_k_values, dtype=np.float32)
    
    print(f"æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: {latent_vectors.shape}")
    
    # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®æ¢ç´¢
    print("\nğŸ¯ æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®æ¢ç´¢...")
    optimal_k, sil_scores, k_range = find_optimal_clusters_simple(latent_vectors)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®é¸æŠã‚’æ±‚ã‚ã‚‹
    final_k = get_user_cluster_choice(optimal_k, sil_scores, k_range)
    
    # ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æçµæœã®å¯è¦–åŒ–ï¼ˆæœ€çµ‚é¸æŠã‚’åæ˜ ï¼‰
    plt.figure(figsize=(10, 6))
    plt.plot(k_range, sil_scores, 'bo-', linewidth=2, markersize=8)
    plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Recommended k = {optimal_k}')
    if final_k != optimal_k:
        plt.axvline(x=final_k, color='green', linestyle='-', linewidth=3, 
                   label=f'Selected k = {final_k}')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Analysis for Optimal k')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å„ç‚¹ã«ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
    for i, (k, score) in enumerate(zip(k_range, sil_scores)):
        color = 'green' if k == final_k else 'red' if k == optimal_k else 'black'
        weight = 'bold' if k in [final_k, optimal_k] else 'normal'
        plt.annotate(f'{score:.3f}', (k, score), 
                   textcoords="offset points", xytext=(0,10), ha='center',
                   color=color, weight=weight)
    
    plt.savefig('silhouette_optimization.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # é¸æŠã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    print(f"\nğŸ“Š K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (k={final_k})...")
    kmeans = KMeans(n_clusters=final_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(latent_vectors)
    
    final_silhouette = silhouette_score(latent_vectors, cluster_labels)
    print(f"æœ€çµ‚ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {final_silhouette:.3f}")
    
    # æ¨å¥¨å€¤ã¨ç•°ãªã‚‹å ´åˆã¯æ¯”è¼ƒã‚’è¡¨ç¤º
    if final_k != optimal_k:
        print(f"\nğŸ“Š æ¯”è¼ƒ:")
        recommended_score = sil_scores[k_range.index(optimal_k)]
        selected_score = sil_scores[k_range.index(final_k)]
        print(f"  æ¨å¥¨ k={optimal_k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {recommended_score:.3f}")
        print(f"  é¸æŠ k={final_k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {selected_score:.3f}")
        diff = selected_score - recommended_score
        trend = "â¬†ï¸" if diff > 0 else "â¬‡ï¸" if diff < 0 else "â¡ï¸"
        print(f"  å·®åˆ†: {diff:+.3f} {trend}")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒã‚’è¡¨ç¤º
    unique, counts = np.unique(cluster_labels, return_counts=True)
    print("\nã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒ:")
    for cluster_id, count in zip(unique, counts):
        percentage = count / len(cluster_labels) * 100
        print(f"  Cluster {cluster_id}: {count} samples ({percentage:.1f}%)")
    
    # æ¬¡å…ƒå‰Šæ¸›ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
    print("\nğŸ“ˆ æ¬¡å…ƒå‰Šæ¸›ä¸­...")
    
    # PCA
    pca = PCA(n_components=2, random_state=42)
    pca_result = pca.fit_transform(latent_vectors)
    
    # t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    tsne_result = tsne.fit_transform(latent_vectors)
    
    # çµæœã‚’ã¾ã¨ã‚ã‚‹
    results = {
        'latent_vectors': latent_vectors,
        'cluster_labels': cluster_labels,
        'filenames': all_filenames,
        'f_values': f_values,
        'k_values': k_values,
        'pca_result': pca_result,
        'tsne_result': tsne_result,
        'n_clusters': final_k,
        'silhouette_score': final_silhouette,
        'optimal_k_analysis': {
            'k_range': k_range,
            'silhouette_scores': sil_scores,
            'recommended_k': optimal_k,
            'selected_k': final_k
        }
    }
    
    # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜
    print("\nğŸ’¾ ä¿å­˜ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_data = {
        'model_state_dict': model.state_dict(),
        'latent_dim': latent_dim,
        'fixed_frames': fixed_frames,
        'target_size': target_size,
        'num_epochs': num_epochs,
        'final_loss': losses[-1],
        'n_clusters': final_k
    }
    torch.save(model_data, 'trained_model.pth')
    
    # åˆ†æçµæœä¿å­˜
    with open('analysis_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    
    # CSVå‡ºåŠ›
    df = pd.DataFrame({
        'filename': all_filenames,
        'f_value': f_values,
        'k_value': k_values,
        'cluster': cluster_labels
    })
    df.to_csv('clustering_results.csv', index=False)
    
    print("âœ… ä¿å­˜å®Œäº†!")
    print(f"  - trained_model.pth: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« ({os.path.getsize('trained_model.pth')/1024/1024:.1f}MB)")
    print(f"  - analysis_results.pkl: åˆ†æçµæœ ({os.path.getsize('analysis_results.pkl')/1024:.1f}KB)")
    print(f"  - clustering_results.csv: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœãƒ†ãƒ¼ãƒ–ãƒ«")
    print(f"  - training_loss.png: å­¦ç¿’æ›²ç·š")
    print(f"  - silhouette_optimization.png: ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æçµæœ")
    
    print(f"\nğŸ‰ å­¦ç¿’å®Œäº†!")
    print(f"ğŸ“Š æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {final_k}")
    print(f"ğŸ“ˆ æœ€çµ‚ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {final_silhouette:.3f}")
    print(f"ğŸ’¾ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)}")

if __name__ == "__main__":
    main() 