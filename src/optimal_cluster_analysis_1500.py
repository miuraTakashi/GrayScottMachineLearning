#!/usr/bin/env python3
"""
1500ã‚µãƒ³ãƒ—ãƒ«ç”¨ æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°è§£æ
ã‚ˆã‚Šå¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§ã®åˆ†æã‚’å®Ÿæ–½
"""

import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import matplotlib as mpl

def load_1500_data():
    """1500ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    
    print("ğŸ“‚ 1500ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    data_file = '../results/latent_representations_frames_all.pkl'
    
    if not os.path.exists(data_file):
        print(f"âŒ {data_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    with open(data_file, 'rb') as f:
        data = pickle.load(f)
    
    latent_vectors = data['latent_vectors']  # (1500, 128)
    f_values = data['f_values']              # 1500å€‹
    k_values = data['k_values']              # 1500å€‹
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(f_values)} ã‚µãƒ³ãƒ—ãƒ«, æ½œåœ¨æ¬¡å…ƒ: {latent_vectors.shape[1]}")
    
    return latent_vectors, f_values, k_values

def evaluate_clustering_range(latent_vectors, min_k=2, max_k=60):
    """å¹…åºƒã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æ"""
    
    print(f"ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•° {min_k}-{max_k} ã§æœ€é©åŒ–åˆ†æä¸­...")
    
    k_range = range(min_k, max_k + 1)
    silhouette_scores = []
    calinski_scores = []
    davies_bouldin_scores = []
    inertias = []
    
    for k in k_range:
        print(f"   k={k:2d}: ", end="", flush=True)
        
        # K-meanså®Ÿè¡Œ
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(latent_vectors)
        
        # å„ç¨®è©•ä¾¡æŒ‡æ¨™
        sil_score = silhouette_score(latent_vectors, cluster_labels)
        cal_score = calinski_harabasz_score(latent_vectors, cluster_labels)
        db_score = davies_bouldin_score(latent_vectors, cluster_labels)
        inertia = kmeans.inertia_
        
        silhouette_scores.append(sil_score)
        calinski_scores.append(cal_score)
        davies_bouldin_scores.append(db_score)
        inertias.append(inertia)
        
        print(f"silhouette={sil_score:.3f}, calinski={cal_score:.1f}, db={db_score:.3f}")
    
    return {
        'k_range': list(k_range),
        'silhouette_scores': silhouette_scores,
        'calinski_scores': calinski_scores,
        'davies_bouldin_scores': davies_bouldin_scores,
        'inertias': inertias
    }

def plot_clustering_analysis(results):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æçµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    
    print("ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æçµæœã‚’å¯è¦–åŒ–ä¸­...")
    
    k_range = results['k_range']
    sil_scores = results['silhouette_scores']
    cal_scores = results['calinski_scores']
    db_scores = results['davies_bouldin_scores']
    inertias = results['inertias']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢
    axes[0, 0].plot(k_range, sil_scores, 'bo-', linewidth=2, markersize=6)
    axes[0, 0].set_xlabel('Number of Clusters (k)')
    axes[0, 0].set_ylabel('Silhouette Score')
    axes[0, 0].set_title('Silhouette Analysis (1500 samples)')
    axes[0, 0].grid(True, alpha=0.3)
    
    # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’ãƒãƒ¼ã‚¯
    best_sil_idx = np.argmax(sil_scores)
    best_sil_k = k_range[best_sil_idx]
    best_sil_score = sil_scores[best_sil_idx]
    axes[0, 0].plot(best_sil_k, best_sil_score, 'ro', markersize=10, 
                    label=f'Best: k={best_sil_k} (score={best_sil_score:.3f})')
    axes[0, 0].legend()
    
    # 2. Calinski-Harabasz Index
    axes[0, 1].plot(k_range, cal_scores, 'go-', linewidth=2, markersize=6)
    axes[0, 1].set_xlabel('Number of Clusters (k)')
    axes[0, 1].set_ylabel('Calinski-Harabasz Index')
    axes[0, 1].set_title('Calinski-Harabasz Analysis')
    axes[0, 1].grid(True, alpha=0.3)
    
    # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’ãƒãƒ¼ã‚¯
    best_cal_idx = np.argmax(cal_scores)
    best_cal_k = k_range[best_cal_idx]
    best_cal_score = cal_scores[best_cal_idx]
    axes[0, 1].plot(best_cal_k, best_cal_score, 'ro', markersize=10,
                    label=f'Best: k={best_cal_k} (score={best_cal_score:.1f})')
    axes[0, 1].legend()
    
    # 3. Davies-Bouldin Index (ä½ã„æ–¹ãŒè‰¯ã„)
    axes[1, 0].plot(k_range, db_scores, 'mo-', linewidth=2, markersize=6)
    axes[1, 0].set_xlabel('Number of Clusters (k)')
    axes[1, 0].set_ylabel('Davies-Bouldin Index')
    axes[1, 0].set_title('Davies-Bouldin Analysis (lower is better)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # æœ€ä½ã‚¹ã‚³ã‚¢ã‚’ãƒãƒ¼ã‚¯
    best_db_idx = np.argmin(db_scores)
    best_db_k = k_range[best_db_idx]
    best_db_score = db_scores[best_db_idx]
    axes[1, 0].plot(best_db_k, best_db_score, 'ro', markersize=10,
                    label=f'Best: k={best_db_k} (score={best_db_score:.3f})')
    axes[1, 0].legend()
    
    # 4. ã‚¨ãƒ«ãƒœãƒ¼æ³•
    axes[1, 1].plot(k_range, inertias, 'co-', linewidth=2, markersize=6)
    axes[1, 1].set_xlabel('Number of Clusters (k)')
    axes[1, 1].set_ylabel('Inertia (Within-cluster sum of squares)')
    axes[1, 1].set_title('Elbow Method')
    axes[1, 1].grid(True, alpha=0.3)
    
    # ã‚¨ãƒ«ãƒœãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æ¨å®š
    elbow_k = estimate_elbow_point(k_range, inertias)
    elbow_inertia = inertias[k_range.index(elbow_k)]
    axes[1, 1].plot(elbow_k, elbow_inertia, 'ro', markersize=10,
                    label=f'Elbow: k={elbow_k}')
    axes[1, 1].legend()
    
    plt.suptitle('Optimal Cluster Analysis for 1500 Samples', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜
    output_file = '../results/optimal_cluster_analysis_1500samples.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… Saved: {output_file}")
    
    return {
        'best_silhouette': (best_sil_k, best_sil_score),
        'best_calinski': (best_cal_k, best_cal_score),
        'best_davies_bouldin': (best_db_k, best_db_score),
        'elbow_point': elbow_k
    }

def estimate_elbow_point(k_range, inertias):
    """ã‚¨ãƒ«ãƒœãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æ¨å®š"""
    
    # äºŒæ¬¡å·®åˆ†ã‚’è¨ˆç®—
    diffs = np.diff(inertias)
    diff2 = np.diff(diffs)
    
    # æœ€å¤§ã®å¤‰åŒ–ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹
    elbow_idx = np.argmax(diff2) + 2  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª¿æ•´
    elbow_k = k_range[elbow_idx] if elbow_idx < len(k_range) else k_range[-1]
    
    return elbow_k

def analyze_top_candidates(latent_vectors, f_values, k_values, candidate_ks):
    """ä¸Šä½å€™è£œã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§è©³ç´°åˆ†æ"""
    
    print(f"\nğŸ“ˆ ä¸Šä½å€™è£œã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è©³ç´°åˆ†æ...")
    
    results = {}
    
    for k in candidate_ks:
        print(f"\nğŸ” k={k} ã®è©³ç´°åˆ†æ:")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(latent_vectors)
        
        # è©•ä¾¡æŒ‡æ¨™
        sil_score = silhouette_score(latent_vectors, cluster_labels)
        cal_score = calinski_harabasz_score(latent_vectors, cluster_labels)
        db_score = davies_bouldin_score(latent_vectors, cluster_labels)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒ
        unique, counts = np.unique(cluster_labels, return_counts=True)
        min_cluster_size = counts.min()
        max_cluster_size = counts.max()
        mean_cluster_size = counts.mean()
        
        print(f"   ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {sil_score:.3f}")
        print(f"   Calinski-Harabasz: {cal_score:.1f}")
        print(f"   Davies-Bouldin: {db_score:.3f}")
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚º: min={min_cluster_size}, max={max_cluster_size}, mean={mean_cluster_size:.1f}")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ã®åˆ†æ
        f_ranges = []
        k_ranges = []
        for cluster in unique:
            mask = cluster_labels == cluster
            f_range = f_values[mask].max() - f_values[mask].min()
            k_range = k_values[mask].max() - k_values[mask].min()
            f_ranges.append(f_range)
            k_ranges.append(k_range)
        
        mean_f_range = np.mean(f_ranges)
        mean_k_range = np.mean(k_ranges)
        print(f"   å¹³å‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²: f={mean_f_range:.4f}, k={mean_k_range:.4f}")
        
        results[k] = {
            'silhouette': sil_score,
            'calinski': cal_score,
            'davies_bouldin': db_score,
            'cluster_sizes': counts,
            'mean_f_range': mean_f_range,
            'mean_k_range': mean_k_range
        }
    
    return results

def print_recommendations(best_metrics, detailed_results):
    """æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’è¡¨ç¤º"""
    
    print(f"\nğŸ¯ 1500ã‚µãƒ³ãƒ—ãƒ«ç”¨ æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°")
    print("=" * 50)
    
    # å„æ‰‹æ³•ã®çµæœ
    sil_k, sil_score = best_metrics['best_silhouette']
    cal_k, cal_score = best_metrics['best_calinski']
    db_k, db_score = best_metrics['best_davies_bouldin']
    elbow_k = best_metrics['elbow_point']
    
    print(f"ğŸ“Š å„æ‰‹æ³•ã«ã‚ˆã‚‹æ¨å¥¨å€¤:")
    print(f"   ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æ: k={sil_k} (ã‚¹ã‚³ã‚¢: {sil_score:.3f})")
    print(f"   Calinski-Harabasz: k={cal_k} (ã‚¹ã‚³ã‚¢: {cal_score:.1f})")
    print(f"   Davies-Bouldin: k={db_k} (ã‚¹ã‚³ã‚¢: {db_score:.3f})")
    print(f"   ã‚¨ãƒ«ãƒœãƒ¼æ³•: k={elbow_k}")
    
    # æ¨å¥¨é †ä½
    candidates = [sil_k, cal_k, db_k, elbow_k]
    unique_candidates = sorted(list(set(candidates)), reverse=True)
    
    print(f"\nğŸ† ç·åˆæ¨å¥¨é †ä½:")
    for i, k in enumerate(unique_candidates[:5]):
        if k in detailed_results:
            result = detailed_results[k]
            print(f"   {i+1}. k={k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆ={result['silhouette']:.3f}, "
                  f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¹³å‡ã‚µã‚¤ã‚º={result['cluster_sizes'].mean():.1f}")
    
    print(f"\nğŸ’¡ 375ã‚µãƒ³ãƒ—ãƒ«æ™‚ä»£ã¨ã®æ¯”è¼ƒ:")
    print(f"   375ã‚µãƒ³ãƒ—ãƒ«: k=20 (ã‚·ãƒ«ã‚¨ãƒƒãƒˆ=0.551)")
    print(f"   1500ã‚µãƒ³ãƒ—ãƒ«: k={sil_k} (ã‚·ãƒ«ã‚¨ãƒƒãƒˆ={sil_score:.3f})")
    
    if sil_k > 20:
        print(f"   ğŸ”º ã‚ˆã‚Šç´°ã‹ã„åˆ†é¡ãŒå¯èƒ½: {sil_k-20}å€‹å¤šã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")
    elif sil_k < 20:
        print(f"   ğŸ”» ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡ãŒæœ€é©: {20-sil_k}å€‹å°‘ãªã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")
    else:
        print(f"   â¡ï¸ åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒæœ€é©")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ” 1500ã‚µãƒ³ãƒ—ãƒ«ç”¨ æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°è§£æ")
    print("=" * 50)
    print(f"ğŸ”§ Matplotlib version: {mpl.__version__}")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    latent_vectors, f_values, k_values = load_1500_data()
    if latent_vectors is None:
        return
    
    # å¹…åºƒã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§è©•ä¾¡
    results = evaluate_clustering_range(latent_vectors, min_k=2, max_k=60)
    
    # çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    best_metrics = plot_clustering_analysis(results)
    
    # ä¸Šä½å€™è£œã®è©³ç´°åˆ†æ
    sil_k = best_metrics['best_silhouette'][0]
    cal_k = best_metrics['best_calinski'][0]
    db_k = best_metrics['best_davies_bouldin'][0]
    elbow_k = best_metrics['elbow_point']
    
    candidates = sorted(list(set([sil_k, cal_k, db_k, elbow_k, 20, 30, 40])))[:7]  # 375æ™‚ä»£ã®20ã‚‚å«ã‚ã‚‹
    detailed_results = analyze_top_candidates(latent_vectors, f_values, k_values, candidates)
    
    # æ¨å¥¨äº‹é …è¡¨ç¤º
    print_recommendations(best_metrics, detailed_results)
    
    print(f"\nğŸ‰ æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°è§£æå®Œäº†!")
    print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: optimal_cluster_analysis_1500samples.png")

if __name__ == "__main__":
    main() 