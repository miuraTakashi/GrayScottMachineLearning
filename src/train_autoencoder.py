#!/usr/bin/env python3
"""
Gray-Scott 3D CNN Autoencoder Training Script
ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å­¦ç¿’ã®ã¿ã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²æŒ‡å®šæ©Ÿèƒ½ä»˜ã
"""

import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import argparse

# ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from gray_scott_autoencoder import GrayScottDataset, Conv3DAutoencoder

def parse_frame_range(frame_range_str):
    """
    ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
    ä¾‹: "10-50", "1-128", "20-80"
    """
    if frame_range_str is None:
        return None
    
    try:
        parts = frame_range_str.split('-')
        if len(parts) != 2:
            raise ValueError("Frame range must be in format 'start-end'")
        
        start = int(parts[0])
        end = int(parts[1])
        
        if start >= end or start < 0:
            raise ValueError("Invalid frame range: start must be < end and >= 0")
        
        return (start, end)
    except ValueError as e:
        raise ValueError(f"Invalid frame range '{frame_range_str}': {e}")

def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(description='Gray-Scott 3D CNN Autoencoder Training with Frame Range Support')
    parser.add_argument('--gif-folder', type=str, default='../data/gif', 
                       help='Path to GIF folder (default: ../data/gif)')
    parser.add_argument('--batch-size', type=int, default=4, 
                       help='Batch size (default: 4)')
    parser.add_argument('--fixed-frames', type=int, default=30, 
                       help='Fixed number of frames (default: 30)')
    parser.add_argument('--frame-range', type=str, default=None, 
                       help='Frame range to use (format: start-end, e.g., "10-50"). If not specified, uses all frames.')
    parser.add_argument('--target-size', type=str, default='64,64', 
                       help='Target image size (format: width,height, default: 64,64)')
    parser.add_argument('--latent-dim', type=int, default=64, 
                       help='Latent dimension (default: 64)')
    parser.add_argument('--epochs', type=int, default=50, 
                       help='Number of epochs (default: 50)')
    parser.add_argument('--lr', type=float, default=0.001, 
                       help='Learning rate (default: 0.001)')
    parser.add_argument('--output-suffix', type=str, default='', 
                       help='Suffix for output files (default: empty)')
    
    args = parser.parse_args()
    
    print("ğŸ”¬ Gray-Scott 3D CNN Autoencoder Training")
    print("ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å­¦ç¿’å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²æŒ‡å®šå¯¾å¿œï¼‰")
    print("=" * 60)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
    gif_folder = args.gif_folder
    batch_size = args.batch_size
    fixed_frames = args.fixed_frames
    frame_range = parse_frame_range(args.frame_range)
    target_size = tuple(map(int, args.target_size.split(',')))
    latent_dim = args.latent_dim
    num_epochs = args.epochs
    learning_rate = args.lr
    output_suffix = args.output_suffix
    
    print("ğŸ“‹ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"  - GIFãƒ•ã‚©ãƒ«ãƒ€: {gif_folder}")
    print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"  - å›ºå®šãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {fixed_frames}")
    if frame_range is not None:
        print(f"  - ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²: {frame_range[0]} - {frame_range[1]} (ç¯„å›²: {frame_range[1] - frame_range[0]}ãƒ•ãƒ¬ãƒ¼ãƒ )")
    else:
        print(f"  - ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²: å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ä½¿ç”¨")
    print(f"  - ç”»åƒã‚µã‚¤ã‚º: {target_size}")
    print(f"  - æ½œåœ¨æ¬¡å…ƒæ•°: {latent_dim}")
    print(f"  - ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
    print(f"  - å­¦ç¿’ç‡: {learning_rate}")
    if output_suffix:
        print(f"  - å‡ºåŠ›ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹: {output_suffix}")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­...")
    dataset = GrayScottDataset(gif_folder, fixed_frames=fixed_frames, target_size=target_size, frame_range=frame_range)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
    
    # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã‚’å–å¾—ï¼ˆæœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ï¼‰
    first_sample = dataset[0]
    sample_tensor = first_sample['tensor']
    print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {sample_tensor.shape}")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\nğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = Conv3DAutoencoder(latent_dim=latent_dim, fixed_frames=fixed_frames, target_size=target_size).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“ˆ ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
    print(f"ğŸ“ˆ å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
    
    # å­¦ç¿’
    print(f"\nğŸš€ å­¦ç¿’é–‹å§‹ ({num_epochs} ã‚¨ãƒãƒƒã‚¯)...")
    model.train()
    losses = []
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        for batch_idx, batch in enumerate(dataloader):
            data = batch['tensor'].to(device)
            
            optimizer.zero_grad()
            reconstructed, _ = model(data)
            loss = criterion(reconstructed, data)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 20 == 0:
                print(f"  Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.6f}")
        
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        print(f"âœ… Epoch {epoch+1}/{num_epochs} å®Œäº†, å¹³å‡Loss: {avg_loss:.6f}")
        
        # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ä¸­é–“ä¿å­˜
        if (epoch + 1) % 10 == 0:
            print(f"ğŸ’¾ ä¸­é–“ä¿å­˜ (Epoch {epoch+1})...")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨­å®š
    if frame_range is not None:
        frame_suffix = f"_frames{frame_range[0]}-{frame_range[1]}"
    else:
        frame_suffix = "_frames_all"
    
    if output_suffix:
        frame_suffix += f"_{output_suffix}"
    
    # å­¦ç¿’æ›²ç·šã‚’ä¿å­˜
    print("\nğŸ“Š å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ä¸­...")
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 1, 1)
    plt.plot(losses, 'b-', linewidth=2)
    if frame_range is not None:
        plt.title(f'Training Loss Over Time (Frames {frame_range[0]}-{frame_range[1]})', fontsize=14)
    else:
        plt.title('Training Loss Over Time (All Frames)', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.grid(True, alpha=0.3)
    
    # ãƒ­ã‚°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã‚‚è¡¨ç¤º
    plt.subplot(2, 1, 2)
    plt.plot(losses, 'r-', linewidth=2)
    plt.title('Training Loss (Log Scale)', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss (log scale)')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'../results/training_loss{frame_suffix}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç‰¹å¾´æŠ½å‡ºï¼ˆæ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
    print("\nğŸ” ç‰¹å¾´æŠ½å‡ºä¸­...")
    model.eval()
    all_latent_vectors = []
    all_filenames = []
    all_f_values = []
    all_k_values = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            data = batch['tensor'].to(device)
            filenames = batch['filename']
            f_vals = batch['f_value']
            k_vals = batch['k_value']
            
            _, latent = model(data)
            all_latent_vectors.append(latent.cpu().numpy())
            all_filenames.extend(filenames)
            all_f_values.extend(f_vals)
            all_k_values.extend(k_vals)
            
            if (batch_idx + 1) % 20 == 0:
                print(f"  ç‰¹å¾´æŠ½å‡ºé€²æ—: {batch_idx+1}/{len(dataloader)} ãƒãƒƒãƒå®Œäº†")
    
    # é…åˆ—ã«å¤‰æ›
    latent_vectors = np.vstack(all_latent_vectors)
    f_values = np.array(all_f_values, dtype=np.float32)
    k_values = np.array(all_k_values, dtype=np.float32)
    
    print(f"ğŸ“ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: {latent_vectors.shape}")
    print(f"ğŸ“Š få€¤ç¯„å›²: {f_values.min():.4f} - {f_values.max():.4f}")
    print(f"ğŸ“Š kå€¤ç¯„å›²: {k_values.min():.4f} - {k_values.max():.4f}")
    
    # æ½œåœ¨ç©ºé–“ã®åˆ†æ
    print("\nğŸ“ˆ æ½œåœ¨ç©ºé–“ã®çµ±è¨ˆåˆ†æ...")
    latent_mean = np.mean(latent_vectors, axis=0)
    latent_std = np.std(latent_vectors, axis=0)
    
    print(f"ğŸ“Š æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«çµ±è¨ˆ:")
    print(f"  - å¹³å‡å€¤ã®ç¯„å›²: {latent_mean.min():.4f} - {latent_mean.max():.4f}")
    print(f"  - æ¨™æº–åå·®ã®ç¯„å›²: {latent_std.min():.4f} - {latent_std.max():.4f}")
    print(f"  - å…¨ä½“å¹³å‡: {np.mean(latent_vectors):.4f}")
    print(f"  - å…¨ä½“æ¨™æº–åå·®: {np.std(latent_vectors):.4f}")
    
    # çµæœã‚’ã¾ã¨ã‚ã‚‹
    autoencoder_results = {
        'latent_vectors': latent_vectors,
        'filenames': all_filenames,
        'f_values': f_values,
        'k_values': k_values,
        'model_config': {
            'latent_dim': latent_dim,
            'fixed_frames': fixed_frames,
            'target_size': target_size,
            'frame_range': frame_range,
            'num_epochs': num_epochs,
            'learning_rate': learning_rate,
            'batch_size': batch_size
        },
        'training_stats': {
            'final_loss': losses[-1],
            'min_loss': min(losses),
            'loss_history': losses,
            'total_samples': len(dataset)
        }
    }
    
    # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜
    print("\nğŸ’¾ ä¿å­˜ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’å«ã‚€ï¼‰
    model_data = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'model_config': {
            'latent_dim': latent_dim,
            'fixed_frames': fixed_frames,
            'target_size': target_size,
            'frame_range': frame_range
        },
        'training_config': {
            'num_epochs': num_epochs,
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'final_loss': losses[-1],
            'loss_history': losses
        },
        'dataset_info': {
            'total_samples': len(dataset),
            'gif_folder': gif_folder
        }
    }
    torch.save(model_data, f'../models/trained_autoencoder{frame_suffix}.pth')
    
    # æ½œåœ¨è¡¨ç¾ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    with open(f'../results/latent_representations{frame_suffix}.pkl', 'wb') as f:
        pickle.dump(autoencoder_results, f)
    
    # CSVå‡ºåŠ›ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å‰ï¼‰
    df = pd.DataFrame({
        'filename': all_filenames,
        'f_value': f_values,
        'k_value': k_values
    })
    df.to_csv(f'../results/extracted_features{frame_suffix}.csv', index=False)
    
    # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºè¡¨ç¤º
    model_size = os.path.getsize(f'../models/trained_autoencoder{frame_suffix}.pth') / 1024 / 1024
    data_size = os.path.getsize(f'../results/latent_representations{frame_suffix}.pkl') / 1024
    csv_size = os.path.getsize(f'../results/extracted_features{frame_suffix}.csv') / 1024
    
    print("âœ… ä¿å­˜å®Œäº†!")
    print(f"  ğŸ“¦ trained_autoencoder{frame_suffix}.pth: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« ({model_size:.1f}MB)")
    print(f"  ğŸ“¦ latent_representations{frame_suffix}.pkl: æ½œåœ¨è¡¨ç¾ãƒ‡ãƒ¼ã‚¿ ({data_size:.1f}KB)")
    print(f"  ğŸ“¦ extracted_features{frame_suffix}.csv: ç‰¹å¾´é‡ãƒ†ãƒ¼ãƒ–ãƒ« ({csv_size:.1f}KB)")
    print(f"  ğŸ“¦ training_loss{frame_suffix}.png: å­¦ç¿’æ›²ç·š")
    
    print(f"\nğŸ‰ ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å­¦ç¿’å®Œäº†!")
    print("=" * 60)
    print("ğŸ“‹ å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  ğŸ¯ æœ€çµ‚æå¤±: {losses[-1]:.6f}")
    print(f"  ğŸ“‰ æœ€å°æå¤±: {min(losses):.6f}")
    print(f"  ğŸ“Š ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)}")
    print(f"  ğŸ”¢ æ½œåœ¨æ¬¡å…ƒæ•°: {latent_dim}")
    print(f"  â±ï¸  ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
    if frame_range is not None:
        print(f"  ğŸ¬ ä½¿ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²: {frame_range[0]} - {frame_range[1]} ({frame_range[1] - frame_range[0]}ãƒ•ãƒ¬ãƒ¼ãƒ )")
    else:
        print(f"  ğŸ¬ ä½¿ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ : å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ")
    
    print(f"\nğŸ”— æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    if frame_range is not None:
        print(f"  1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‚’å®Ÿè¡Œ: python cluster_analysis.py --latent-file latent_representations{frame_suffix}.pkl")
        print(f"  2. ã¾ãŸã¯åŒ…æ‹¬çš„åˆ†æ: python optimal_clustering.py --latent-file latent_representations{frame_suffix}.pkl")
    else:
        print("  1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‚’å®Ÿè¡Œ: python cluster_analysis.py")
        print("  2. ã¾ãŸã¯åŒ…æ‹¬çš„åˆ†æ: python optimal_clustering.py")

if __name__ == "__main__":
    main() 