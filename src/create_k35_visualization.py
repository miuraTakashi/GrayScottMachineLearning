#!/usr/bin/env python3
"""
k=35 è©³ç´°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§ã®1500ã‚µãƒ³ãƒ—ãƒ«å¯è¦–åŒ–
ç´°ã‹ã„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ã§ã®çµ±åˆå¯è¦–åŒ–
"""

import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from matplotlib.patches import Patch
import matplotlib as mpl
from matplotlib.colors import ListedColormap
import seaborn as sns

def load_and_process_k35_data():
    """1500ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’k=35ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
    
    print("ğŸ“‚ 1500ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    data_file = '../results/latent_representations_frames_all.pkl'
    
    if not os.path.exists(data_file):
        print(f"âŒ {data_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    with open(data_file, 'rb') as f:
        data = pickle.load(f)
    
    latent_vectors = data['latent_vectors']  # (1500, 128)
    filenames = data['filenames']            # 1500å€‹
    f_values = data['f_values']              # 1500å€‹
    k_values = data['k_values']              # 1500å€‹
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(filenames)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # k=35ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    print("ğŸ” k=35ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    n_clusters = 35
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(latent_vectors)
    silhouette = silhouette_score(latent_vectors, cluster_labels)
    print(f"   ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {silhouette:.3f}")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒè¡¨ç¤º
    unique, counts = np.unique(cluster_labels, return_counts=True)
    print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒçµ±è¨ˆ:")
    print(f"     å¹³å‡ã‚µãƒ³ãƒ—ãƒ«æ•°: {counts.mean():.1f}")
    print(f"     æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼: {counts.max()} samples")
    print(f"     æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼: {counts.min()} samples")
    print(f"     æ¨™æº–åå·®: {counts.std():.1f}")
    
    # ã‚µã‚¤ã‚ºã®å¤§ãã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’è¡¨ç¤º
    large_clusters = [(i, count) for i, count in enumerate(counts) if count > counts.mean() + counts.std()]
    if large_clusters:
        print(f"   å¤§ããªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼:")
        for cluster_id, count in large_clusters:
            print(f"     Cluster {cluster_id}: {count} samples ({count/len(cluster_labels)*100:.1f}%)")
    
    # æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œ
    print("ğŸ“‰ æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œä¸­...")
    pca = PCA(n_components=2, random_state=42)
    pca_result = pca.fit_transform(latent_vectors)
    print(f"   PCAå¯„ä¸ç‡: PC1={pca.explained_variance_ratio_[0]:.3f}, PC2={pca.explained_variance_ratio_[1]:.3f}")
    
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    tsne_result = tsne.fit_transform(latent_vectors)
    print("   t-SNEå®Œäº†")
    
    return {
        'f_values': f_values,
        'k_values': k_values,
        'cluster_labels': cluster_labels,
        'pca_result': pca_result,
        'tsne_result': tsne_result,
        'silhouette_score': silhouette,
        'n_samples': len(filenames),
        'cluster_counts': counts,
        'n_clusters': n_clusters
    }

def create_k35_visualization(results, figsize=(16, 12), dpi=300, save_name='gray_scott_clustering_results_k35_1500samples.png'):
    """k=35ã§ã®çµ±åˆå¯è¦–åŒ–ï¼ˆ1500ã‚µãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    
    f_values = results['f_values']
    k_values = results['k_values']
    cluster_labels = results['cluster_labels']
    latent_2d_pca = results['pca_result']
    latent_2d_tsne = results['tsne_result']
    n_clusters = results['n_clusters']
    
    print(f"ğŸ¨ k=35çµ±åˆå¯è¦–åŒ–ä½œæˆä¸­ (1500ã‚µãƒ³ãƒ—ãƒ«)...")
    
    # k=35ç”¨ã®é€£ç¶šã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ä½¿ç”¨
    try:
        colormap = plt.cm.viridis
    except:
        try:
            colormap = plt.cm.tab20
        except:
            colormap = plt.cm.Set3
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # 1. f-kç©ºé–“ã§ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
    scatter1 = axes[0, 0].scatter(f_values, k_values, c=cluster_labels, 
                                  cmap=colormap, alpha=0.7, s=20)
    axes[0, 0].set_xlabel('f parameter', fontsize=12)
    axes[0, 0].set_ylabel('k parameter', fontsize=12)
    axes[0, 0].set_title('Clustering Results in f-k Space\n(k=35, 1500 samples)', fontsize=14, fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].invert_yaxis()
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒå¤šã„ã®ã§ï¼‰
    plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster ID', shrink=0.8)
    
    # 2. PCAå¯è¦–åŒ–
    scatter2 = axes[0, 1].scatter(latent_2d_pca[:, 0], latent_2d_pca[:, 1], 
                                  c=cluster_labels, cmap=colormap, alpha=0.7, s=20)
    axes[0, 1].set_xlabel('PCA Component 1', fontsize=12)
    axes[0, 1].set_ylabel('PCA Component 2', fontsize=12)
    axes[0, 1].set_title('PCA Visualization of Latent Space\n(k=35, 1500 samples)', fontsize=14, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    plt.colorbar(scatter2, ax=axes[0, 1], label='Cluster ID', shrink=0.8)
    
    # 3. t-SNEå¯è¦–åŒ–
    scatter3 = axes[1, 0].scatter(latent_2d_tsne[:, 0], latent_2d_tsne[:, 1], 
                                  c=cluster_labels, cmap=colormap, alpha=0.7, s=20)
    axes[1, 0].set_xlabel('t-SNE Component 1', fontsize=12)
    axes[1, 0].set_ylabel('t-SNE Component 2', fontsize=12)
    axes[1, 0].set_title('t-SNE Visualization of Latent Space\n(k=35, 1500 samples)', fontsize=14, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    plt.colorbar(scatter3, ax=axes[1, 0], label='Cluster ID', shrink=0.8)
    
    # 4. f-kå¹³é¢ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆk=35å°‚ç”¨ï¼‰
    f_unique = np.unique(f_values)
    k_unique = np.unique(k_values)
    
    heatmap_data = np.full((len(f_unique), len(k_unique)), np.nan)
    
    for i, f in enumerate(f_values):
        k = k_values[i]
        cluster = cluster_labels[i]
        
        f_idx = np.where(f_unique == f)[0][0]
        k_idx = np.where(k_unique == k)[0][0]
        heatmap_data[f_idx, k_idx] = cluster
    
    # k=35ç”¨ã®é€£ç¶šã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
    im = axes[1, 1].imshow(heatmap_data, cmap=colormap, aspect='auto', origin='upper', 
                           vmin=0, vmax=n_clusters-1)
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è»¸è¨­å®š
    step_k = max(1, len(k_unique) // 8)
    step_f = max(1, len(f_unique) // 8)
    
    axes[1, 1].set_xticks(range(0, len(k_unique), step_k))
    axes[1, 1].set_yticks(range(0, len(f_unique), step_f))
    axes[1, 1].set_xticklabels([f'{k:.4f}' for k in k_unique[::step_k]], rotation=45, fontsize=9)
    axes[1, 1].set_yticklabels([f'{f:.4f}' for f in f_unique[::step_f]], fontsize=9)
    axes[1, 1].set_xlabel('k parameter', fontsize=12)
    axes[1, 1].set_ylabel('f parameter', fontsize=12)
    axes[1, 1].set_title('Cluster Heatmap in f-k Space\n(k=35, 1500 samples)', fontsize=14, fontweight='bold')
    axes[1, 1].invert_yaxis()
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ã®ã‚«ãƒ©ãƒ¼ãƒãƒ¼
    cbar = plt.colorbar(im, ax=axes[1, 1], shrink=0.6)
    cbar.set_label('Cluster ID', fontsize=10)
    
    # å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«
    fig.suptitle(f'Gray-Scott Clustering Results (k=35, 1500 samples)\nSilhouette Score: {results["silhouette_score"]:.3f}', 
                 fontsize=16, fontweight='bold', y=0.95)
    
    plt.tight_layout()
    
    # ä¿å­˜ãƒ‘ã‚¹ã®ä¿®æ­£
    if not save_name.startswith('../results/'):
        save_name = f'../results/{save_name}'
    
    plt.savefig(save_name, dpi=dpi, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… Saved: {save_name}")

def analyze_k35_clusters(results):
    """k=35ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è©³ç´°åˆ†æ"""
    
    print(f"\nğŸ“Š k=35ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è©³ç´°åˆ†æ")
    print("=" * 60)
    
    f_values = results['f_values']
    k_values = results['k_values']
    cluster_labels = results['cluster_labels']
    cluster_counts = results['cluster_counts']
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print(f"ğŸ”¢ çµ±è¨ˆã‚µãƒãƒªãƒ¼:")
    print(f"   ç·ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {len(cluster_counts)}")
    print(f"   å¹³å‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚º: {cluster_counts.mean():.1f} Â± {cluster_counts.std():.1f}")
    print(f"   æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼: {cluster_counts.max()} samples")
    print(f"   æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼: {cluster_counts.min()} samples")
    print(f"   ä¸­å¤®å€¤: {np.median(cluster_counts):.1f}")
    
    # å¤§ããªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆå¹³å‡+æ¨™æº–åå·®ä»¥ä¸Šï¼‰ã®åˆ†æ
    threshold = cluster_counts.mean() + cluster_counts.std()
    large_clusters = []
    
    print(f"\nğŸ¯ ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ (>{threshold:.0f} samples):")
    
    for cluster in range(len(cluster_counts)):
        if cluster_counts[cluster] > threshold:
            mask = cluster_labels == cluster
            count = np.sum(mask)
            
            f_mean = f_values[mask].mean()
            k_mean = k_values[mask].mean()
            f_std = f_values[mask].std()
            k_std = k_values[mask].std()
            
            large_clusters.append({
                'id': cluster,
                'count': count,
                'f_mean': f_mean,
                'k_mean': k_mean,
                'f_std': f_std,
                'k_std': k_std
            })
            
            print(f"   Cluster {cluster:2d}: {count:3d} samples ({count/len(cluster_labels)*100:.1f}%)")
            print(f"      f: {f_mean:.4f} Â± {f_std:.4f}")
            print(f"      k: {k_mean:.4f} Â± {k_std:.4f}")
    
    # f-kç©ºé–“ã§ã®åˆ†å¸ƒåˆ†æ
    print(f"\nğŸŒ f-kç©ºé–“åˆ†å¸ƒåˆ†æ:")
    f_ranges = []
    k_ranges = []
    
    for cluster in range(len(cluster_counts)):
        mask = cluster_labels == cluster
        if np.sum(mask) > 5:  # æœ€ä½5ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã¿
            f_range = f_values[mask].max() - f_values[mask].min()
            k_range = k_values[mask].max() - k_values[mask].min()
            f_ranges.append(f_range)
            k_ranges.append(k_range)
    
    print(f"   få€¤ç¯„å›²ã®å¹³å‡: {np.mean(f_ranges):.4f}")
    print(f"   kå€¤ç¯„å›²ã®å¹³å‡: {np.mean(k_ranges):.4f}")
    print(f"   ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {len([r for r in f_ranges if r < 0.01])} / {len(f_ranges)}")

def create_k35_heatmap(results, figsize=(12, 8), save_name='gray_scott_k35_heatmap_1500samples.png'):
    """k=35å°‚ç”¨ã®f-kç©ºé–“ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
    
    f_values = results['f_values']
    k_values = results['k_values']
    cluster_labels = results['cluster_labels']
    
    print(f"ğŸ—ºï¸  k=35å°‚ç”¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆä¸­...")
    
    # f-kç©ºé–“ã®ã‚°ãƒªãƒƒãƒ‰ä½œæˆ
    f_unique = np.unique(f_values)
    k_unique = np.unique(k_values)
    
    heatmap_data = np.full((len(f_unique), len(k_unique)), np.nan)
    
    for i, f in enumerate(f_values):
        k = k_values[i]
        cluster = cluster_labels[i]
        
        f_idx = np.where(f_unique == f)[0][0]
        k_idx = np.where(k_unique == k)[0][0]
        heatmap_data[f_idx, k_idx] = cluster
    
    fig, ax = plt.subplots(figsize=figsize)
    
    # k=35ç”¨ã®é€£ç¶šã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
    im = ax.imshow(heatmap_data, cmap='viridis', aspect='auto', origin='upper', vmin=0, vmax=34)
    
    # è»¸è¨­å®š
    step_k = max(1, len(k_unique) // 12)
    step_f = max(1, len(f_unique) // 12)
    
    ax.set_xticks(range(0, len(k_unique), step_k))
    ax.set_yticks(range(0, len(f_unique), step_f))
    ax.set_xticklabels([f'{k:.4f}' for k in k_unique[::step_k]], rotation=45, fontsize=10)
    ax.set_yticklabels([f'{f:.4f}' for f in f_unique[::step_f]], fontsize=10)
    ax.set_xlabel('k parameter', fontsize=12)
    ax.set_ylabel('f parameter', fontsize=12)
    ax.set_title(f'Detailed Cluster Heatmap in f-k Space\n(k=35, 1500 samples)', fontsize=14, fontweight='bold')
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
    cbar = plt.colorbar(im, ax=ax, shrink=0.8)
    cbar.set_label('Cluster ID', fontsize=12)
    
    plt.tight_layout()
    
    # ä¿å­˜
    if not save_name.startswith('../results/'):
        save_name = f'../results/{save_name}'
    
    plt.savefig(save_name, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… Heatmap saved: {save_name}")

def print_k35_comparison():
    """k=35ã¨ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã¨ã®æ¯”è¼ƒ"""
    
    print(f"\nğŸ“ˆ k=35 vs ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°æ¯”è¼ƒ")
    print("=" * 60)
    print(f"ğŸ¯ k=35ã®ç‰¹å¾´:")
    print(f"   âœ… éå¸¸ã«è©³ç´°ãªãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡")
    print(f"   âœ… å¹³å‡43ã‚µãƒ³ãƒ—ãƒ«/ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰")
    print(f"   âœ… ç´°ã‹ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é•ã„ã‚’æ¤œå‡º")
    print(f"   âš ï¸  è§£é‡ˆã®è¤‡é›‘ã•ãŒå¢—åŠ ")
    
    print(f"\nğŸ”„ ä»–ã®é¸æŠè‚¢ã¨ã®ä½ç½®ã¥ã‘:")
    print(f"   k=2:  å¤§ã¾ã‹ãªäºŒåˆ†é¡ï¼ˆæœ€é«˜ã‚·ãƒ«ã‚¨ãƒƒãƒˆï¼‰")
    print(f"   k=4:  ãƒãƒ©ãƒ³ã‚¹é‡è¦–ï¼ˆã‚¨ãƒ«ãƒœãƒ¼æ³•æ¨å¥¨ï¼‰")
    print(f"   k=20: å¾“æ¥ã®375ã‚µãƒ³ãƒ—ãƒ«ç›¸å½“")
    print(f"   k=35: è©³ç´°ç ”ç©¶ç”¨ï¼ˆç¾åœ¨ï¼‰")
    print(f"   k=50+: éç´°åˆ†åŒ–ã®å±é™ºæ€§")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ¨ Gray-Scott k=35è©³ç´°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¯è¦–åŒ–")
    print("=" * 60)
    print(f"ğŸ”§ Matplotlib version: {mpl.__version__}")
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    results = load_and_process_k35_data()
    if results is None:
        return
    
    # k=35çµ±åˆå¯è¦–åŒ–ä½œæˆ
    create_k35_visualization(results)
    
    # k=35å°‚ç”¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    create_k35_heatmap(results)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è©³ç´°åˆ†æ
    analyze_k35_clusters(results)
    
    # æ¯”è¼ƒæƒ…å ±è¡¨ç¤º
    print_k35_comparison()
    
    print(f"\nğŸ‰ k=35å¯è¦–åŒ–å®Œäº†!")
    print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   - gray_scott_clustering_results_k35_1500samples.png")
    print(f"   - gray_scott_k35_heatmap_1500samples.png")
    print(f"ğŸ”¬ è©³ç´°ãª35ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main() 