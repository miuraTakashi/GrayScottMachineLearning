{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Gray-Scott Phase 4: Last 64 Frames GPU版（Google Colab）\n",
    "\n",
    "**目標**: 後半64フレームのみを使用した学習でPhase 1 (0.565) → Phase 4 (0.65+) へのさらなる向上\n",
    "\n",
    "**主要改善点**:\n",
    "- ✅ 残差接続（ResNet）+ 時空間注意機構\n",
    "- ✅ GPU高速化（CPU比 5-10倍）\n",
    "- ✅ Google Drive連携\n",
    "- ✅ **後半64フレームのみを使用** - より安定した後期パターンに焦点\n",
    "\n",
    "**前提条件**: Google Driveの`マイドライブ/GrayScottML/gif/`に1500個のGIFファイルを配置\n",
    "\n",
    "**実行時間**: **GPU 3-5分** 🏃‍♂️💨\n",
    "\n",
    "## 更新履歴\n",
    "- 2025-08-11: Data augmentation（平行移動・90/180/270度回転・水平/垂直反転）を導入。\n",
    "  - 学習: GrayScottDatasetLast64Aug(augment=True) でオンザフライ適用\n",
    "  - 評価/潜在ベクトル抽出: augment=False でデータ分布を固定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Step 1: 環境セットアップ & Google Drive接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick run mode: use only first N GIF files\n",
    "QUICK_RUN = True\n",
    "max_samples = 20 if QUICK_RUN else None\n",
    "print('Quick run max_samples =', max_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded Phase 4 model code (no external src import needed)\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Gray-Scott 3D CNN Autoencoder - Phase 4: 対比学習・評価改善\n",
    "目標: Phase 3 (0.5144) → Phase 4 (0.55+) への更なる向上\n",
    "\n",
    "主要改善点:\n",
    "- 対比学習（Contrastive Learning）\n",
    "- 階層的クラスタリング分析\n",
    "- 包括的評価指標\n",
    "- 改善された訓練ループ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import imageio.v2 as imageio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Phase 4 Using device: {device}\")\n",
    "\n",
    "# ================================\n",
    "# 1. 対比学習システム\n",
    "# ================================\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"f-kパラメータ類似性に基づく対比学習損失\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.5, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.margin = margin\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "    \n",
    "    def forward(self, features, f_params, k_params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (batch_size, feature_dim) - 潜在表現\n",
    "            f_params: (batch_size,) - fパラメータ\n",
    "            k_params: (batch_size,) - kパラメータ\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # f-kパラメータ空間での類似性計算\n",
    "        f_diff = torch.abs(f_params.unsqueeze(1) - f_params.unsqueeze(0))\n",
    "        k_diff = torch.abs(k_params.unsqueeze(1) - k_params.unsqueeze(0))\n",
    "        \n",
    "        # 正規化されたパラメータ距離\n",
    "        param_distance = torch.sqrt(f_diff**2 + k_diff**2)\n",
    "        \n",
    "        # 類似性閾値（近いパラメータは類似、遠いパラメータは非類似）\n",
    "        similarity_threshold = 0.01  # f-k空間での閾値\n",
    "        positive_mask = param_distance < similarity_threshold\n",
    "        negative_mask = param_distance > similarity_threshold * 3\n",
    "        \n",
    "        # 特徴量の類似性計算\n",
    "        features_norm = F.normalize(features, p=2, dim=1)\n",
    "        similarity_matrix = torch.mm(features_norm, features_norm.t()) / self.temperature\n",
    "        \n",
    "        # 対比学習損失\n",
    "        positive_loss = 0\n",
    "        negative_loss = 0\n",
    "        \n",
    "        if positive_mask.sum() > 0:\n",
    "            positive_sim = similarity_matrix[positive_mask]\n",
    "            positive_loss = -torch.log(torch.exp(positive_sim).sum() / torch.exp(similarity_matrix).sum())\n",
    "        \n",
    "        if negative_mask.sum() > 0:\n",
    "            negative_sim = similarity_matrix[negative_mask]\n",
    "            negative_loss = torch.log(torch.exp(negative_sim).sum() / torch.exp(similarity_matrix).sum())\n",
    "        \n",
    "        contrastive_loss = positive_loss + negative_loss\n",
    "        \n",
    "        return contrastive_loss\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"対比学習用射影ヘッド\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.BatchNorm1d(output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "# ================================\n",
    "# 2. 階層的クラスタリング分析\n",
    "# ================================\n",
    "\n",
    "class HierarchicalClusteringAnalysis:\n",
    "    \"\"\"階層的クラスタリング分析システム\"\"\"\n",
    "    \n",
    "    def __init__(self, method='ward', metric='euclidean'):\n",
    "        self.method = method\n",
    "        self.metric = metric\n",
    "        self.linkage_matrix = None\n",
    "        self.optimal_clusters = None\n",
    "    \n",
    "    def fit(self, features):\n",
    "        \"\"\"階層的クラスタリング実行\"\"\"\n",
    "        # 特徴量の標準化\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # 階層的クラスタリング\n",
    "        self.linkage_matrix = linkage(features_scaled, method=self.method, metric=self.metric)\n",
    "        \n",
    "        # 最適クラスタ数の決定\n",
    "        self.optimal_clusters = self._find_optimal_clusters(features_scaled)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _find_optimal_clusters(self, features, max_clusters=20):\n",
    "        \"\"\"最適クラスタ数の自動決定\"\"\"\n",
    "        silhouette_scores = []\n",
    "        cluster_range = range(2, min(max_clusters + 1, len(features) // 2))\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            cluster_labels = fcluster(self.linkage_matrix, n_clusters, criterion='maxclust')\n",
    "            \n",
    "            if len(np.unique(cluster_labels)) > 1:\n",
    "                silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "            else:\n",
    "                silhouette_scores.append(-1)\n",
    "        \n",
    "        if silhouette_scores:\n",
    "            optimal_idx = np.argmax(silhouette_scores)\n",
    "            optimal_n_clusters = cluster_range[optimal_idx]\n",
    "            return optimal_n_clusters\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    def get_cluster_labels(self, n_clusters=None):\n",
    "        \"\"\"クラスタラベルの取得\"\"\"\n",
    "        if n_clusters is None:\n",
    "            n_clusters = self.optimal_clusters\n",
    "        \n",
    "        return fcluster(self.linkage_matrix, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    def plot_dendrogram(self, figsize=(12, 8)):\n",
    "        \"\"\"デンドログラム可視化\"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        dendrogram(self.linkage_matrix, truncate_mode='level', p=10)\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.show()\n",
    "\n",
    "# ================================\n",
    "# 3. 包括的評価指標\n",
    "# ================================\n",
    "\n",
    "class ComprehensiveEvaluationMetrics:\n",
    "    \"\"\"包括的評価指標システム\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def calculate_all_metrics(self, features, labels, f_params=None, k_params=None):\n",
    "        \"\"\"全ての評価指標を計算\"\"\"\n",
    "        \n",
    "        # 基本クラスタリング指標\n",
    "        self.metrics['silhouette_score'] = silhouette_score(features, labels)\n",
    "        self.metrics['calinski_harabasz_score'] = calinski_harabasz_score(features, labels)\n",
    "        self.metrics['davies_bouldin_score'] = davies_bouldin_score(features, labels)\n",
    "        \n",
    "        # 近傍一致度指標\n",
    "        self.metrics['neighborhood_agreement'] = self._calculate_neighborhood_agreement(features, labels)\n",
    "        \n",
    "        # パラメータ空間分離度評価\n",
    "        if f_params is not None and k_params is not None:\n",
    "            self.metrics['parameter_separation'] = self._calculate_parameter_separation(\n",
    "                features, labels, f_params, k_params\n",
    "            )\n",
    "        \n",
    "        # クラスタ内分散・クラスタ間分散\n",
    "        self.metrics['within_cluster_variance'] = self._calculate_within_cluster_variance(features, labels)\n",
    "        self.metrics['between_cluster_variance'] = self._calculate_between_cluster_variance(features, labels)\n",
    "        \n",
    "        # 安定性指標\n",
    "        self.metrics['cluster_stability'] = self._calculate_cluster_stability(features, labels)\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def _calculate_neighborhood_agreement(self, features, labels, k=10):\n",
    "        \"\"\"近傍一致度の計算\"\"\"\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(features)\n",
    "        distances, indices = nbrs.kneighbors(features)\n",
    "        \n",
    "        agreements = []\n",
    "        for i in range(len(features)):\n",
    "            neighbor_labels = labels[indices[i][1:]]  # 自分以外の近傍\n",
    "            same_cluster = np.sum(neighbor_labels == labels[i])\n",
    "            agreement = same_cluster / k\n",
    "            agreements.append(agreement)\n",
    "        \n",
    "        return np.mean(agreements)\n",
    "    \n",
    "    def _calculate_parameter_separation(self, features, labels, f_params, k_params):\n",
    "        \"\"\"パラメータ空間分離度の計算\"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        separations = []\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            if np.sum(mask) > 1:\n",
    "                cluster_f = f_params[mask]\n",
    "                cluster_k = k_params[mask]\n",
    "                \n",
    "                # クラスタ内のパラメータ分散\n",
    "                f_var = np.var(cluster_f)\n",
    "                k_var = np.var(cluster_k)\n",
    "                cluster_variance = f_var + k_var\n",
    "                \n",
    "                separations.append(cluster_variance)\n",
    "        \n",
    "        return np.mean(separations) if separations else 0\n",
    "    \n",
    "    def _calculate_within_cluster_variance(self, features, labels):\n",
    "        \"\"\"クラスタ内分散の計算\"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        within_variances = []\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            if np.sum(mask) > 1:\n",
    "                cluster_features = features[mask]\n",
    "                centroid = np.mean(cluster_features, axis=0)\n",
    "                variance = np.mean(np.sum((cluster_features - centroid)**2, axis=1))\n",
    "                within_variances.append(variance)\n",
    "        \n",
    "        return np.mean(within_variances) if within_variances else 0\n",
    "    \n",
    "    def _calculate_between_cluster_variance(self, features, labels):\n",
    "        \"\"\"クラスタ間分散の計算\"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        centroids = []\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            centroid = np.mean(features[mask], axis=0)\n",
    "            centroids.append(centroid)\n",
    "        \n",
    "        centroids = np.array(centroids)\n",
    "        overall_centroid = np.mean(centroids, axis=0)\n",
    "        \n",
    "        between_variance = np.mean(np.sum((centroids - overall_centroid)**2, axis=1))\n",
    "        return between_variance\n",
    "    \n",
    "    def _calculate_cluster_stability(self, features, labels, n_bootstrap=10):\n",
    "        \"\"\"クラスタ安定性の計算\"\"\"\n",
    "        from sklearn.utils import resample\n",
    "        \n",
    "        original_labels = labels\n",
    "        stability_scores = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # ブートストラップサンプリング\n",
    "            bootstrap_indices = resample(range(len(features)), n_samples=len(features))\n",
    "            bootstrap_features = features[bootstrap_indices]\n",
    "            bootstrap_labels = labels[bootstrap_indices]\n",
    "            \n",
    "            # クラスタリング実行\n",
    "            kmeans = KMeans(n_clusters=len(np.unique(original_labels)), random_state=42)\n",
    "            new_labels = kmeans.fit_predict(bootstrap_features)\n",
    "            \n",
    "            # ラベル一致度計算（ハンガリアンアルゴリズム簡易版）\n",
    "            agreement = self._calculate_label_agreement(bootstrap_labels, new_labels)\n",
    "            stability_scores.append(agreement)\n",
    "        \n",
    "        return np.mean(stability_scores)\n",
    "    \n",
    "    def _calculate_label_agreement(self, labels1, labels2):\n",
    "        \"\"\"ラベル一致度の計算\"\"\"\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        \n",
    "        unique_labels1 = np.unique(labels1)\n",
    "        unique_labels2 = np.unique(labels2)\n",
    "        \n",
    "        # 混同行列作成\n",
    "        confusion_matrix = np.zeros((len(unique_labels1), len(unique_labels2)))\n",
    "        \n",
    "        for i, label1 in enumerate(unique_labels1):\n",
    "            for j, label2 in enumerate(unique_labels2):\n",
    "                confusion_matrix[i, j] = np.sum((labels1 == label1) & (labels2 == label2))\n",
    "        \n",
    "        # ハンガリアンアルゴリズムで最適割り当て\n",
    "        row_ind, col_ind = linear_sum_assignment(-confusion_matrix)\n",
    "        \n",
    "        # 一致度計算\n",
    "        agreement = confusion_matrix[row_ind, col_ind].sum() / len(labels1)\n",
    "        return agreement\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        \"\"\"評価指標の表示\"\"\"\n",
    "        print(\"\\n🎯 Phase 4 包括的評価指標\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 基本指標\n",
    "        print(f\"Silhouette Score: {self.metrics.get('silhouette_score', 0):.4f}\")\n",
    "        print(f\"Calinski-Harabasz: {self.metrics.get('calinski_harabasz_score', 0):.2f}\")\n",
    "        print(f\"Davies-Bouldin: {self.metrics.get('davies_bouldin_score', 0):.4f}\")\n",
    "        \n",
    "        # 高度な指標\n",
    "        print(f\"Neighborhood Agreement: {self.metrics.get('neighborhood_agreement', 0):.4f}\")\n",
    "        print(f\"Parameter Separation: {self.metrics.get('parameter_separation', 0):.4f}\")\n",
    "        print(f\"Within Cluster Variance: {self.metrics.get('within_cluster_variance', 0):.4f}\")\n",
    "        print(f\"Between Cluster Variance: {self.metrics.get('between_cluster_variance', 0):.4f}\")\n",
    "        print(f\"Cluster Stability: {self.metrics.get('cluster_stability', 0):.4f}\")\n",
    "\n",
    "# ================================\n",
    "# 4. Phase 3ベースのモデル拡張\n",
    "# ================================\n",
    "\n",
    "class GrayScottAugmentation:\n",
    "    \"\"\"Gray-Scott専用データ拡張クラス（Phase 3から継承）\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 temporal_shift_prob=0.3,\n",
    "                 spatial_flip_prob=0.5,\n",
    "                 noise_prob=0.2,\n",
    "                 intensity_prob=0.3,\n",
    "                 temporal_crop_prob=0.2):\n",
    "        self.temporal_shift_prob = temporal_shift_prob\n",
    "        self.spatial_flip_prob = spatial_flip_prob\n",
    "        self.noise_prob = noise_prob\n",
    "        self.intensity_prob = intensity_prob\n",
    "        self.temporal_crop_prob = temporal_crop_prob\n",
    "    \n",
    "    def temporal_shift(self, tensor, max_shift=3):\n",
    "        \"\"\"時間軸シフト\"\"\"\n",
    "        if np.random.random() < self.temporal_shift_prob:\n",
    "            shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "            if shift != 0:\n",
    "                tensor = torch.roll(tensor, shift, dims=1)\n",
    "        return tensor\n",
    "    \n",
    "    def spatial_flip(self, tensor):\n",
    "        \"\"\"空間軸反転\"\"\"\n",
    "        if np.random.random() < self.spatial_flip_prob:\n",
    "            if np.random.random() < 0.5:\n",
    "                tensor = torch.flip(tensor, dims=[3])\n",
    "            if np.random.random() < 0.5:\n",
    "                tensor = torch.flip(tensor, dims=[2])\n",
    "        return tensor\n",
    "    \n",
    "    def add_noise(self, tensor, noise_std=0.02):\n",
    "        \"\"\"ガウシアンノイズ追加\"\"\"\n",
    "        if np.random.random() < self.noise_prob:\n",
    "            noise = torch.randn_like(tensor) * noise_std\n",
    "            tensor = torch.clamp(tensor + noise, 0, 1)\n",
    "        return tensor\n",
    "    \n",
    "    def intensity_transform(self, tensor, gamma_range=(0.8, 1.2)):\n",
    "        \"\"\"強度変換\"\"\"\n",
    "        if np.random.random() < self.intensity_prob:\n",
    "            gamma = np.random.uniform(*gamma_range)\n",
    "            tensor = torch.pow(tensor, gamma)\n",
    "        return tensor\n",
    "    \n",
    "    def temporal_crop(self, tensor, crop_ratio=0.1):\n",
    "        \"\"\"時間軸クロップ\"\"\"\n",
    "        if np.random.random() < self.temporal_crop_prob:\n",
    "            T = tensor.shape[1]\n",
    "            crop_size = int(T * crop_ratio)\n",
    "            start_idx = np.random.randint(0, crop_size + 1)\n",
    "            end_idx = T - np.random.randint(0, crop_size + 1)\n",
    "            \n",
    "            cropped = tensor[:, start_idx:end_idx]\n",
    "            tensor = F.interpolate(cropped.unsqueeze(0), size=(T, tensor.shape[2], tensor.shape[3]), \n",
    "                                 mode='trilinear', align_corners=False).squeeze(0)\n",
    "        return tensor\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"全ての拡張を適用\"\"\"\n",
    "        tensor = self.temporal_shift(tensor)\n",
    "        tensor = self.spatial_flip(tensor)\n",
    "        tensor = self.add_noise(tensor)\n",
    "        tensor = self.intensity_transform(tensor)\n",
    "        tensor = self.temporal_crop(tensor)\n",
    "        return tensor\n",
    "\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    \"\"\"マルチスケール特徴融合（Phase 3から継承）\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 4つの異なるスケールでの特徴抽出\n",
    "        self.scale1 = nn.Conv3d(in_channels, out_channels // 4, kernel_size=1, padding=0)  # Point-wise\n",
    "        self.scale2 = nn.Conv3d(in_channels, out_channels // 4, kernel_size=3, padding=1)  # Local\n",
    "        self.scale3 = nn.Conv3d(in_channels, out_channels // 4, kernel_size=5, padding=2)  # Global\n",
    "        \n",
    "        # プーリングベースの特徴\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.scale4 = nn.Conv3d(in_channels, out_channels // 4, kernel_size=1)\n",
    "        \n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 各スケールで特徴抽出\n",
    "        feat1 = self.scale1(x)\n",
    "        feat2 = self.scale2(x)\n",
    "        feat3 = self.scale3(x)\n",
    "        \n",
    "        # プーリング特徴\n",
    "        pooled = self.pool(x)\n",
    "        feat4 = self.scale4(pooled)\n",
    "        feat4 = feat4.expand_as(feat1)\n",
    "        \n",
    "        # 特徴融合\n",
    "        fused = torch.cat([feat1, feat2, feat3, feat4], dim=1)\n",
    "        fused = self.bn(fused)\n",
    "        fused = self.relu(fused)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "class EnhancedSpatioTemporalAttention(nn.Module):\n",
    "    \"\"\"改良時空間注意機構（Phase 3から継承）\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 時間注意\n",
    "        self.temporal_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1)),\n",
    "            nn.Conv3d(in_channels, in_channels // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels // reduction, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 空間注意\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1, None, None)),\n",
    "            nn.Conv3d(in_channels, in_channels // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels // reduction, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # チャネル注意\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Conv3d(in_channels, in_channels // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels // reduction, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 各注意機構を適用\n",
    "        temp_att = self.temporal_attention(x)\n",
    "        spat_att = self.spatial_attention(x)\n",
    "        chan_att = self.channel_attention(x)\n",
    "        \n",
    "        # 注意重みを適用\n",
    "        x = x * temp_att * spat_att * chan_att\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ResidualMultiScaleBlock3D(nn.Module):\n",
    "    \"\"\"残差マルチスケールブロック（Phase 3から継承）\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_scale = MultiScaleFeatureFusion(in_channels, out_channels)\n",
    "        self.attention = EnhancedSpatioTemporalAttention(out_channels)\n",
    "        \n",
    "        # 残差接続用\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        \n",
    "        out = self.multi_scale(x)\n",
    "        out = self.attention(out)\n",
    "        \n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class Conv3DAutoencoderPhase4(nn.Module):\n",
    "    \"\"\"Phase 4: 対比学習統合オートエンコーダー\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=512, input_shape=(20, 64, 64)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # エンコーダー\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 入力: (1, 20, 64, 64)\n",
    "            ResidualMultiScaleBlock3D(1, 32),\n",
    "            nn.MaxPool3d(2),  # (32, 10, 32, 32)\n",
    "            \n",
    "            ResidualMultiScaleBlock3D(32, 64),\n",
    "            nn.MaxPool3d(2),  # (64, 5, 16, 16)\n",
    "            \n",
    "            ResidualMultiScaleBlock3D(64, 128),\n",
    "            nn.MaxPool3d(2),  # (128, 2, 8, 8)\n",
    "            \n",
    "            ResidualMultiScaleBlock3D(128, 256),\n",
    "            nn.AdaptiveAvgPool3d(1),  # (256, 1, 1, 1)\n",
    "        )\n",
    "        \n",
    "        # 潜在空間\n",
    "        self.fc_encoder = nn.Sequential(\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 対比学習用射影ヘッド\n",
    "        self.projection_head = ProjectionHead(latent_dim, 256, 128)\n",
    "        \n",
    "        # デコーダー\n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # デコーダーを個別のレイヤーとして定義（サイズ調整付き）\n",
    "        self.decoder_conv1 = nn.ConvTranspose3d(256, 128, kernel_size=(3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.decoder_bn1 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        self.decoder_conv2 = nn.ConvTranspose3d(128, 64, kernel_size=(3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.decoder_bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        self.decoder_conv3 = nn.ConvTranspose3d(64, 32, kernel_size=(3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.decoder_bn3 = nn.BatchNorm3d(32)\n",
    "        \n",
    "        self.decoder_conv4 = nn.ConvTranspose3d(32, 1, kernel_size=(3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"エンコード\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        latent = self.fc_encoder(x)\n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        \"\"\"デコード（サイズ調整付き）\"\"\"\n",
    "        x = self.fc_decoder(latent)\n",
    "        x = x.view(x.size(0), 256, 1, 1, 1)\n",
    "        \n",
    "        # 段階的にデコード（各ステップでサイズを調整）\n",
    "        # (256, 1, 1, 1) -> (128, 2, 8, 8)\n",
    "        x = self.decoder_conv1(x)\n",
    "        x = self.decoder_bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # サイズ調整\n",
    "        x = F.interpolate(x, size=(2, 8, 8), mode='trilinear', align_corners=False)\n",
    "        \n",
    "        # (128, 2, 8, 8) -> (64, 5, 16, 16)\n",
    "        x = self.decoder_conv2(x)\n",
    "        x = self.decoder_bn2(x)\n",
    "        x = self.relu(x)\n",
    "        # サイズ調整\n",
    "        x = F.interpolate(x, size=(5, 16, 16), mode='trilinear', align_corners=False)\n",
    "        \n",
    "        # (64, 5, 16, 16) -> (32, 10, 32, 32)\n",
    "        x = self.decoder_conv3(x)\n",
    "        x = self.decoder_bn3(x)\n",
    "        x = self.relu(x)\n",
    "        # サイズ調整\n",
    "        x = F.interpolate(x, size=(10, 32, 32), mode='trilinear', align_corners=False)\n",
    "        \n",
    "        # (32, 10, 32, 32) -> (1, 20, 64, 64)\n",
    "        x = self.decoder_conv4(x)\n",
    "        # 最終サイズ調整\n",
    "        x = F.interpolate(x, size=(20, 64, 64), mode='trilinear', align_corners=False)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"フォワードパス\"\"\"\n",
    "        latent = self.encode(x)\n",
    "        reconstructed = self.decode(latent)\n",
    "        projection = self.projection_head(latent)\n",
    "        \n",
    "        return reconstructed, latent, projection\n",
    "\n",
    "# ================================\n",
    "# 5. データセットクラス\n",
    "# ================================\n",
    "\n",
    "class GrayScottDataset(Dataset):\n",
    "    \"\"\"Gray-Scott データセット（Phase 3から継承・拡張）\"\"\"\n",
    "    \n",
    "    def __init__(self, gif_folder, augmentation=None, max_samples=None):\n",
    "        self.gif_folder = gif_folder\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        # GIFファイルリスト取得\n",
    "        self.gif_files = [f for f in os.listdir(gif_folder) if f.endswith('.gif')]\n",
    "        \n",
    "        if max_samples:\n",
    "            self.gif_files = self.gif_files[:max_samples]\n",
    "        \n",
    "        # f-kパラメータ抽出\n",
    "        self.f_params = []\n",
    "        self.k_params = []\n",
    "        \n",
    "        for gif_file in self.gif_files:\n",
    "            f_val, k_val = self.extract_parameters(gif_file)\n",
    "            self.f_params.append(f_val)\n",
    "            self.k_params.append(k_val)\n",
    "        \n",
    "        self.f_params = np.array(self.f_params)\n",
    "        self.k_params = np.array(self.k_params)\n",
    "        \n",
    "        print(f\"📊 Dataset loaded: {len(self.gif_files)} samples\")\n",
    "        print(f\"f range: {self.f_params.min():.4f} - {self.f_params.max():.4f}\")\n",
    "        print(f\"k range: {self.k_params.min():.4f} - {self.k_params.max():.4f}\")\n",
    "    \n",
    "    def extract_parameters(self, filename):\n",
    "        \"\"\"ファイル名からf-kパラメータを抽出\"\"\"\n",
    "        pattern = r'f([\\d.]+)-k([\\d.]+)'\n",
    "        match = re.search(pattern, filename)\n",
    "        \n",
    "        if match:\n",
    "            f_val = float(match.group(1))\n",
    "            k_val = float(match.group(2))\n",
    "            return f_val, k_val\n",
    "        else:\n",
    "            return 0.0, 0.0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gif_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gif_path = os.path.join(self.gif_folder, self.gif_files[idx])\n",
    "        \n",
    "        # GIF読み込み\n",
    "        gif = imageio.mimread(gif_path)\n",
    "        \n",
    "        # 最初の20フレームを取得\n",
    "        frames = gif[:20] if len(gif) >= 20 else gif\n",
    "        \n",
    "        # テンソルに変換\n",
    "        tensor = torch.FloatTensor(frames).unsqueeze(0)  # (1, T, H, W)\n",
    "        tensor = tensor / 255.0  # 正規化\n",
    "        \n",
    "        # データ拡張\n",
    "        if self.augmentation:\n",
    "            tensor = self.augmentation(tensor)\n",
    "        \n",
    "        return tensor, self.f_params[idx], self.k_params[idx], idx\n",
    "\n",
    "# ================================\n",
    "# 6. 改善された訓練ループ\n",
    "# ================================\n",
    "\n",
    "def train_phase4_model(model, dataloader, num_epochs=30, learning_rate=1e-4):\n",
    "    \"\"\"Phase 4モデルの訓練\"\"\"\n",
    "    \n",
    "    # 最適化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # 損失関数\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    contrastive_loss = ContrastiveLoss(temperature=0.5)\n",
    "    \n",
    "    # 訓練ループ\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    print(\"🚀 Phase 4 Training Started\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_recon_loss = 0.0\n",
    "        epoch_contrastive_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, f_params, k_params, _) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            f_params = f_params.to(device)\n",
    "            k_params = k_params.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # フォワードパス\n",
    "            reconstructed, latent, projection = model(data)\n",
    "            \n",
    "            # 損失計算\n",
    "            recon_loss = reconstruction_loss(reconstructed, data)\n",
    "            contrast_loss = contrastive_loss(projection, f_params, k_params)\n",
    "            \n",
    "            # 総損失（重み付き）\n",
    "            total_loss = recon_loss + 0.1 * contrast_loss\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_contrastive_loss += contrast_loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(dataloader)\n",
    "        avg_contrast_loss = epoch_contrastive_loss / len(dataloader)\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"  Total Loss: {avg_loss:.6f}\")\n",
    "            print(f\"  Reconstruction: {avg_recon_loss:.6f}\")\n",
    "            print(f\"  Contrastive: {avg_contrast_loss:.6f}\")\n",
    "            print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    print(\"✅ Phase 4 Training Completed!\")\n",
    "    return train_losses\n",
    "\n",
    "# ================================\n",
    "# 7. メイン実行関数\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Phase 4メイン実行\"\"\"\n",
    "    \n",
    "    # データフォルダ設定\n",
    "    GIF_FOLDER = \"path/to/gif/folder\"  # 実際のパスに変更\n",
    "    \n",
    "    if not os.path.exists(GIF_FOLDER):\n",
    "        print(f\"❌ GIF folder not found: {GIF_FOLDER}\")\n",
    "        return\n",
    "    \n",
    "    # データ拡張設定\n",
    "    augmentation = GrayScottAugmentation()\n",
    "    \n",
    "    # データセット作成\n",
    "    dataset = GrayScottDataset(GIF_FOLDER, augmentation=augmentation)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # モデル作成\n",
    "    model = Conv3DAutoencoderPhase4(latent_dim=512).to(device)\n",
    "    \n",
    "    print(f\"📊 Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 訓練実行\n",
    "    train_losses = train_phase4_model(model, dataloader, num_epochs=30)\n",
    "    \n",
    "    # モデル保存\n",
    "    torch.save(model.state_dict(), 'models/phase4_model.pth')\n",
    "    print(\"💾 Model saved to models/phase4_model.pth\")\n",
    "    \n",
    "    # 評価実行\n",
    "    evaluate_phase4_model(model, dataloader)\n",
    "\n",
    "def evaluate_phase4_model(model, dataloader):\n",
    "    \"\"\"Phase 4モデルの評価\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_latents = []\n",
    "    all_f_params = []\n",
    "    all_k_params = []\n",
    "    \n",
    "    print(\"🔍 Phase 4 Evaluation Started\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, f_params, k_params, _ in dataloader:\n",
    "            data = data.to(device)\n",
    "            _, latent, _ = model(data)\n",
    "            \n",
    "            all_latents.append(latent.cpu().numpy())\n",
    "            all_f_params.append(f_params.numpy())\n",
    "            all_k_params.append(k_params.numpy())\n",
    "    \n",
    "    # データ統合\n",
    "    all_latents = np.vstack(all_latents)\n",
    "    all_f_params = np.concatenate(all_f_params)\n",
    "    all_k_params = np.concatenate(all_k_params)\n",
    "    \n",
    "    # 階層的クラスタリング\n",
    "    hierarchical_clustering = HierarchicalClusteringAnalysis()\n",
    "    hierarchical_clustering.fit(all_latents)\n",
    "    \n",
    "    # クラスタラベル取得\n",
    "    cluster_labels = hierarchical_clustering.get_cluster_labels()\n",
    "    \n",
    "    # 包括的評価\n",
    "    evaluator = ComprehensiveEvaluationMetrics()\n",
    "    metrics = evaluator.calculate_all_metrics(\n",
    "        all_latents, cluster_labels, all_f_params, all_k_params\n",
    "    )\n",
    "    \n",
    "    # 結果表示\n",
    "    evaluator.print_metrics()\n",
    "    \n",
    "    # 可視化\n",
    "    visualize_phase4_results(all_latents, cluster_labels, all_f_params, all_k_params)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_phase4_results(latents, labels, f_params, k_params):\n",
    "    \"\"\"Phase 4結果の可視化\"\"\"\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    latents_pca = pca.fit_transform(latents)\n",
    "    \n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(latents)//4))\n",
    "    latents_tsne = tsne.fit_transform(latents)\n",
    "    \n",
    "    # 可視化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # PCA可視化\n",
    "    scatter = axes[0, 0].scatter(latents_pca[:, 0], latents_pca[:, 1], c=labels, cmap='tab10', s=30)\n",
    "    axes[0, 0].set_title('PCA Visualization')\n",
    "    axes[0, 0].set_xlabel('PC1')\n",
    "    axes[0, 0].set_ylabel('PC2')\n",
    "    plt.colorbar(scatter, ax=axes[0, 0])\n",
    "    \n",
    "    # t-SNE可視化\n",
    "    scatter = axes[0, 1].scatter(latents_tsne[:, 0], latents_tsne[:, 1], c=labels, cmap='tab10', s=30)\n",
    "    axes[0, 1].set_title('t-SNE Visualization')\n",
    "    axes[0, 1].set_xlabel('t-SNE 1')\n",
    "    axes[0, 1].set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter, ax=axes[0, 1])\n",
    "    \n",
    "    # f-k空間可視化\n",
    "    scatter = axes[1, 0].scatter(f_params, k_params, c=labels, cmap='tab10', s=30)\n",
    "    axes[1, 0].set_title('f-k Parameter Space')\n",
    "    axes[1, 0].set_xlabel('f parameter')\n",
    "    axes[1, 0].set_ylabel('k parameter')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # クラスタ分布\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    axes[1, 1].bar(unique_labels, counts)\n",
    "    axes[1, 1].set_title('Cluster Distribution')\n",
    "    axes[1, 1].set_xlabel('Cluster')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/phase4_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import imageio.v2 as imageio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# GPU確認\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "    # GPU最適化設定\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "else:\n",
    "    print(\"⚠️ GPU not available. Please enable GPU in Runtime > Change runtime type\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# パッケージインストール\n",
    "try:\n",
    "    import imageio\n",
    "    import seaborn\n",
    "    print(\"✅ All packages available\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing required packages...\")\n",
    "    !pip install imageio scikit-learn seaborn\n",
    "    import imageio\n",
    "    import seaborn\n",
    "    print(\"✅ Packages installed successfully\")\n",
    "\n",
    "# 追加パッケージ確認\n",
    "try:\n",
    "    import umap\n",
    "    import hdbscan\n",
    "    print(\"✅ UMAP/HDBSCAN available\")\n",
    "except Exception:\n",
    "    print(\"📦 Installing umap-learn and hdbscan...\")\n",
    "    !pip install umap-learn hdbscan\n",
    "    import umap\n",
    "    import hdbscan\n",
    "    print(\"✅ UMAP/HDBSCAN installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive接続とデータパス確認\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Driveをマウント\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# データパス設定\n",
    "GIF_FOLDER_PATH = '/content/drive/MyDrive/GrayScottML/gif'\n",
    "\n",
    "# データ確認\n",
    "if os.path.exists(GIF_FOLDER_PATH):\n",
    "    gif_files = [f for f in os.listdir(GIF_FOLDER_PATH) if f.endswith('.gif')]\n",
    "    gif_count = len(gif_files)\n",
    "    print(f\"✅ Google Drive connected successfully!\")\n",
    "    print(f\"📁 Path: {GIF_FOLDER_PATH}\")\n",
    "    print(f\"🎬 GIF files found: {gif_count}\")\n",
    "    \n",
    "    if gif_count >= 1000:\n",
    "        print(\"🎉 Ready for Phase 2 training with last 64 frames!\")\n",
    "    else:\n",
    "        print(f\"⚠️ Not enough files. Expected: 1500, Found: {gif_count}\")\n",
    "        print(\"Please upload more GIF files to Google Drive.\")\n",
    "else:\n",
    "    print(\"❌ Google Drive path not found!\")\n",
    "    print(f\"Expected path: {GIF_FOLDER_PATH}\")\n",
    "    print(\"Please ensure the following structure exists:\")\n",
    "    print(\"  MyDrive/\")\n",
    "    print(\"  └── GrayScottML/\")\n",
    "    print(\"      └── gif/\")\n",
    "    print(\"          ├── GrayScott-f0.0100-k0.0400-00.gif\")\n",
    "    print(\"          └── ... (1500 files)\")\n",
    "    \n",
    "    # マイドライブの内容を表示\n",
    "    mydrive_path = '/content/drive/MyDrive'\n",
    "    if os.path.exists(mydrive_path):\n",
    "        print(f\"\\n📂 Contents of MyDrive:\")\n",
    "        for item in sorted(os.listdir(mydrive_path))[:10]:\n",
    "            print(f\"   📁 {item}\")\n",
    "        print(\"   ... (showing first 10 items)\")\n",
    "    raise FileNotFoundError(\"Please set up the correct folder structure in Google Drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットクラス（後半64フレーム + Augmentation）\n",
    "class GrayScottDatasetLast64Aug(Dataset):\n",
    "    def __init__(self, gif_folder, fixed_frames=64, target_size=(64, 64), max_samples=None, augment=True, max_shift_ratio=0.1):\n",
    "        self.gif_folder = gif_folder\n",
    "        self.fixed_frames = fixed_frames\n",
    "        self.target_size = target_size\n",
    "        self.max_samples = max_samples\n",
    "        self.augment = augment\n",
    "        self.max_shift_ratio = max_shift_ratio\n",
    "        \n",
    "        self.gif_files = []\n",
    "        self.f_values = []\n",
    "        self.k_values = []\n",
    "        self.tensors = []\n",
    "        \n",
    "        self._load_data()\n",
    "    \n",
    "    def _parse_filename(self, filename):\n",
    "        pattern = r\"GrayScott-f([0-9.]+)-k([0-9.]+)-\\d+\\.gif\"\n",
    "        m = re.match(pattern, filename)\n",
    "        if m:\n",
    "            return float(m.group(1)), float(m.group(2))\n",
    "        return None, None\n",
    "    \n",
    "    def _load_gif_as_tensor(self, gif_path):\n",
    "        try:\n",
    "            gif = imageio.mimread(gif_path)\n",
    "            total_frames = len(gif)\n",
    "            if total_frames >= self.fixed_frames:\n",
    "                start_idx = total_frames - self.fixed_frames\n",
    "                selected_frames = gif[start_idx:]\n",
    "            else:\n",
    "                selected_frames = gif\n",
    "            frames = []\n",
    "            for frame in selected_frames:\n",
    "                if len(frame.shape) == 3:\n",
    "                    frame = np.mean(frame, axis=2)\n",
    "                pil_frame = Image.fromarray(frame.astype(np.uint8))\n",
    "                pil_frame = pil_frame.resize(self.target_size)\n",
    "                frame_array = np.array(pil_frame) / 255.0\n",
    "                frames.append(frame_array)\n",
    "            while len(frames) < self.fixed_frames:\n",
    "                frames.append(frames[-1] if frames else np.zeros(self.target_size))\n",
    "            frames = frames[:self.fixed_frames]\n",
    "            tensor = torch.FloatTensor(np.array(frames))  # (T, H, W)\n",
    "            return tensor.unsqueeze(0)  # (1, T, H, W)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {gif_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_data(self):\n",
    "        gif_files = [f for f in os.listdir(self.gif_folder) if f.endswith('.gif')]\n",
    "        if self.max_samples:\n",
    "            gif_files = gif_files[:self.max_samples]\n",
    "        print(f\"Loading {len(gif_files)} GIF files (last 64 frames each) with augmentation={self.augment}...\")\n",
    "        for i, filename in enumerate(gif_files):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Progress: {i+1}/{len(gif_files)} ({(i+1)/len(gif_files)*100:.1f}%)\")\n",
    "            f_val, k_val = self._parse_filename(filename)\n",
    "            if f_val is None or k_val is None:\n",
    "                continue\n",
    "            gif_path = os.path.join(self.gif_folder, filename)\n",
    "            tensor = self._load_gif_as_tensor(gif_path)\n",
    "            if tensor is not None:\n",
    "                self.gif_files.append(filename)\n",
    "                self.f_values.append(f_val)\n",
    "                self.k_values.append(k_val)\n",
    "                self.tensors.append(tensor)\n",
    "        print(f\"✅ Successfully loaded {len(self.tensors)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "    \n",
    "    def _random_flip(self, x):\n",
    "        # x: (1, T, H, W)\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = torch.flip(x, dims=[-1])  # horizontal\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = torch.flip(x, dims=[-2])  # vertical\n",
    "        return x\n",
    "    \n",
    "    def _random_rotate_90(self, x):\n",
    "        k = np.random.choice([0,1,2,3])\n",
    "        if k:\n",
    "            x = torch.rot90(x, k=k, dims=(-2, -1))\n",
    "        return x\n",
    "    \n",
    "    def _random_translate(self, x):\n",
    "        # zero-padded translation by up to max_shift_ratio of size\n",
    "        _, _, H, W = x.shape\n",
    "        max_dx = int(W * self.max_shift_ratio)\n",
    "        max_dy = int(H * self.max_shift_ratio)\n",
    "        if max_dx == 0 and max_dy == 0:\n",
    "            return x\n",
    "        dx = int(np.random.randint(-max_dx, max_dx+1))\n",
    "        dy = int(np.random.randint(-max_dy, max_dy+1))\n",
    "        if dx == 0 and dy == 0:\n",
    "            return x\n",
    "        pad_left = max(dx, 0)\n",
    "        pad_right = max(-dx, 0)\n",
    "        pad_top = max(dy, 0)\n",
    "        pad_bottom = max(-dy, 0)\n",
    "        x_padded = F.pad(x, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0.0)\n",
    "        x_cropped = x_padded[:, :, pad_bottom:pad_bottom+H, pad_right:pad_right+W]\n",
    "        return x_cropped\n",
    "    \n",
    "    def _apply_augmentation(self, x):\n",
    "        x = self._random_flip(x)\n",
    "        x = self._random_rotate_90(x)\n",
    "        x = self._random_translate(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.tensors[idx].clone()  # (1, T, H, W)\n",
    "        if self.augment:\n",
    "            sample = self._apply_augmentation(sample)\n",
    "        return {\n",
    "            'tensor': sample,\n",
    "            'f_value': self.f_values[idx],\n",
    "            'k_value': self.k_values[idx],\n",
    "            'filename': self.gif_files[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットクラス（後半64フレーム専用）\n",
    "class GrayScottDatasetLast64(Dataset):\n",
    "    def __init__(self, gif_folder, fixed_frames=64, target_size=(64, 64), max_samples=None):\n",
    "        self.gif_folder = gif_folder\n",
    "        self.fixed_frames = fixed_frames\n",
    "        self.target_size = target_size\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        self.gif_files = []\n",
    "        self.f_values = []\n",
    "        self.k_values = []\n",
    "        self.tensors = []\n",
    "        \n",
    "        self._load_data()\n",
    "    \n",
    "    def _parse_filename(self, filename):\n",
    "        pattern = r'GrayScott-f([0-9.]+)-k([0-9.]+)-\\d+\\.gif'\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            return float(match.group(1)), float(match.group(2))\n",
    "        return None, None\n",
    "    \n",
    "    def _load_gif_as_tensor(self, gif_path):\n",
    "        try:\n",
    "            gif = imageio.mimread(gif_path)\n",
    "            total_frames = len(gif)\n",
    "            \n",
    "            # 後半64フレームを抽出\n",
    "            if total_frames >= self.fixed_frames:\n",
    "                # 後半64フレームを取得\n",
    "                start_idx = total_frames - self.fixed_frames\n",
    "                selected_frames = gif[start_idx:]\n",
    "            else:\n",
    "                # フレーム数が64未満の場合は全フレームを使用\n",
    "                selected_frames = gif\n",
    "                print(f\"Warning: Only {total_frames} frames available, using all frames\")\n",
    "            \n",
    "            frames = []\n",
    "            for frame in selected_frames:\n",
    "                if len(frame.shape) == 3:\n",
    "                    frame = np.mean(frame, axis=2)\n",
    "                \n",
    "                pil_frame = Image.fromarray(frame.astype(np.uint8))\n",
    "                pil_frame = pil_frame.resize(self.target_size)\n",
    "                frame_array = np.array(pil_frame) / 255.0\n",
    "                frames.append(frame_array)\n",
    "            \n",
    "            # 64フレームになるようにパディング（必要に応じて）\n",
    "            while len(frames) < self.fixed_frames:\n",
    "                frames.append(frames[-1] if frames else np.zeros(self.target_size))\n",
    "            \n",
    "            # 正確に64フレームにする\n",
    "            frames = frames[:self.fixed_frames]\n",
    "            \n",
    "            tensor = torch.FloatTensor(np.array(frames))\n",
    "            return tensor.unsqueeze(0)  # チャンネル次元を追加\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {gif_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_data(self):\n",
    "        gif_files = [f for f in os.listdir(self.gif_folder) if f.endswith('.gif')]\n",
    "        \n",
    "        if self.max_samples:\n",
    "            gif_files = gif_files[:self.max_samples]\n",
    "        \n",
    "        print(f\"Loading {len(gif_files)} GIF files (last 64 frames each)...\")\n",
    "        \n",
    "        for i, filename in enumerate(gif_files):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Progress: {i+1}/{len(gif_files)} ({(i+1)/len(gif_files)*100:.1f}%)\")\n",
    "            \n",
    "            f_val, k_val = self._parse_filename(filename)\n",
    "            if f_val is None or k_val is None:\n",
    "                continue\n",
    "            \n",
    "            gif_path = os.path.join(self.gif_folder, filename)\n",
    "            tensor = self._load_gif_as_tensor(gif_path)\n",
    "            \n",
    "            if tensor is not None:\n",
    "                self.gif_files.append(filename)\n",
    "                self.f_values.append(f_val)\n",
    "                self.k_values.append(k_val)\n",
    "                self.tensors.append(tensor)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded {len(self.tensors)} samples with last 64 frames each\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'tensor': self.tensors[idx],\n",
    "            'f_value': self.f_values[idx],\n",
    "            'k_value': self.k_values[idx],\n",
    "            'filename': self.gif_files[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意機構モジュール\n",
    "class SpatioTemporalAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 空間注意\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1)),\n",
    "            nn.Conv3d(channels, channels//4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(channels//4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 時間注意\n",
    "        self.temporal_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1, None, None)),\n",
    "            nn.Conv3d(channels, channels//4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(channels//4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # チャンネル注意\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Conv3d(channels, channels//4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(channels//4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # 3つの注意機構を適用\n",
    "        x = x * self.spatial_attention(x)\n",
    "        x = x * self.temporal_attention(x)\n",
    "        x = x * self.channel_attention(x)\n",
    "        \n",
    "        return x + identity * 0.1\n",
    "\n",
    "# 残差注意ブロック\n",
    "class ResidualAttentionBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels, momentum=0.1)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels, momentum=0.1)\n",
    "        \n",
    "        self.attention = SpatioTemporalAttention(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels, momentum=0.1)\n",
    "            )\n",
    "        \n",
    "        self.dropout = nn.Dropout3d(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.attention(out)\n",
    "        \n",
    "        out += self.shortcut(identity)\n",
    "        return F.relu(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Step 3: Phase 2 学習実行（64フレーム対応）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 architecture\n",
    "# 利用するモデル: src/gray_scott_autoencoder_phase3.py の Conv3DAutoencoderPhase3 を使用します。\n",
    "# using embedded Conv3DAutoencoderPhase4 (import removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 メインモデル（64フレーム対応）\n",
    "class Conv3DAutoencoderPhase4Last64(nn.Module):\n",
    "    def __init__(self, input_channels=1, fixed_frames=64, target_size=(64, 64), latent_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.fixed_frames = fixed_frames\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # 初期畳み込み（64フレーム対応）\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, 32, (5, 7, 7), (2, 2, 2), (2, 3, 3)),\n",
    "            nn.BatchNorm3d(32, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(0.05)\n",
    "        )\n",
    "        \n",
    "        # 残差注意ブロック群（64フレーム用に調整）\n",
    "        self.res_block1 = ResidualAttentionBlock3D(32, 64, stride=(2, 2, 2))\n",
    "        self.res_block2 = ResidualAttentionBlock3D(64, 64)\n",
    "        self.res_block3 = ResidualAttentionBlock3D(64, 128, stride=(2, 2, 2))\n",
    "        self.res_block4 = ResidualAttentionBlock3D(128, 128)\n",
    "        self.res_block5 = ResidualAttentionBlock3D(128, 256, stride=(2, 2, 2))\n",
    "        self.res_block6 = ResidualAttentionBlock3D(256, 256)\n",
    "        \n",
    "        # グローバルプーリング\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d((2, 2, 2))\n",
    "        self.dropout_before_latent = nn.Dropout3d(0.3)\n",
    "        \n",
    "        # 潜在空間射影\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 2 * 2 * 2, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "        \n",
    "        # 復元\n",
    "        self.from_latent = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256 * 2 * 2 * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # デコーダー（64フレーム復元用）\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, (4, 4, 4), (2, 2, 2), (1, 1, 1)),\n",
    "            nn.BatchNorm3d(128, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(0.2),\n",
    "            \n",
    "            nn.ConvTranspose3d(128, 64, (4, 4, 4), (2, 2, 2), (1, 1, 1)),\n",
    "            nn.BatchNorm3d(64, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(0.15),\n",
    "            \n",
    "            nn.ConvTranspose3d(64, 32, (4, 4, 4), (2, 2, 2), (1, 1, 1)),\n",
    "            nn.BatchNorm3d(32, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(0.1),\n",
    "            \n",
    "            nn.ConvTranspose3d(32, input_channels, (6, 7, 7), (2, 2, 2), (2, 3, 3)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        x = self.res_block6(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout_before_latent(x)\n",
    "        return self.to_latent(x)\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        x = self.from_latent(latent)\n",
    "        x = x.view(-1, 256, 2, 2, 2)\n",
    "        x = self.decoder(x)\n",
    "        target_h, target_w = self.target_size\n",
    "        return F.interpolate(x, size=(self.fixed_frames, target_h, target_w), \n",
    "                           mode='trilinear', align_corners=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        reconstructed = self.decode(latent)\n",
    "        return reconstructed, latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 学習・評価実行（64フレーム版）\n",
    "def run_phase4_training_last64():\n",
    "    # パラメータ設定\n",
    "    fixed_frames = 64  # 後半64フレーム\n",
    "    target_size = (64, 64)\n",
    "    latent_dim = 512\n",
    "    num_epochs = 50\n",
    "    batch_size = 6 if torch.cuda.is_available() else 3  # 64フレームなので少し小さく\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    n_clusters = 5\n",
    "    \n",
    "    print(\"🔄 Creating dataset (last 64 frames, with augmentation)...\")\n",
    "    dataset = GrayScottDatasetLast64Aug(GIF_FOLDER_PATH, fixed_frames, target_size, augment=True, max_samples=max_samples)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "    \n",
    "    print(f\"📊 Dataset: {len(dataset)} samples, Batch size: {batch_size}, Frames: {fixed_frames}\")\n",
    "    \n",
    "    print(\"🧠 Creating Phase 4 model (64 frames)...\")\n",
    "    model = Conv3DAutoencoderPhase4(latent_dim=latent_dim, \n",
    "                                         fixed_frames=fixed_frames, \n",
    "                                         target_size=target_size).to(device)\n",
    "    \n",
    "    print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 訓練\n",
    "    print(\"🎯 Starting training with last 64 frames...\")\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"🚀 Phase 4 GPU Training: ResNet + Attention (Last 64 Frames)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            tensors = batch['tensor'].to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, latent = model(tensors)\n",
    "            loss = criterion(reconstructed, tensors)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # 進捗表示\n",
    "        progress = ((epoch + 1) / num_epochs) * 100\n",
    "        if (epoch + 1) % 5 == 0 or epoch < 5:\n",
    "            print(f'Epoch [{epoch+1:2d}/{num_epochs}] '\n",
    "                  f'({progress:5.1f}%) | '\n",
    "                  f'Loss: {avg_loss:.6f} | '\n",
    "                  f'LR: {current_lr:.2e} | '\n",
    "                  f'Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"🎉 Training completed in {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    # 学習曲線表示\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, linewidth=2, color='purple')\n",
    "    plt.title('Phase 4: GPU Training Loss (ResNet + Attention, Last 64 Frames)', fontsize=14)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # 潜在ベクトル抽出\n",
    "    print(\"🔍 Extracting latent vectors...\")\n",
    "    # Rebuild dataset without augmentation for evaluation/feature extraction\n",
    "    dataset_eval = GrayScottDatasetLast64Aug(GIF_FOLDER_PATH, fixed_frames, target_size, augment=False, max_samples=max_samples)\n",
    "    dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "    f_values = []\n",
    "    k_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_eval:\n",
    "            tensors = batch['tensor'].to(device)\n",
    "            _, latent = model(tensors)\n",
    "            latent_vectors.append(latent.cpu().numpy())\n",
    "            f_values.extend(batch['f_value'].numpy())\n",
    "            k_values.extend(batch['k_value'].numpy())\n",
    "    \n",
    "    latent_vectors = np.vstack(latent_vectors)\n",
    "    f_values = np.array(f_values)\n",
    "    k_values = np.array(k_values)\n",
    "    \n",
    "    # クラスタリング\n",
    "    print(\"�� Performing clustering with multiple strategies...\")\n",
    "from sklearn.decomposition import PCA as _PCA\n",
    "from sklearn.preprocessing import StandardScaler as _SS\n",
    "from sklearn.cluster import KMeans as _KMeans\n",
    "# Strategy A: PCA(whiten=True) -> KMeans(n_init=50)\n",
    "Xa = _PCA(n_components=min(64, latent_vectors.shape[1]), whiten=True, random_state=42).fit_transform(latent_vectors)\n",
    "labels_a = _KMeans(n_clusters=n_clusters, n_init=50, random_state=42).fit_predict(Xa)\n",
    "sil_a = silhouette_score(latent_vectors, labels_a)\n",
    "print(f\"[A] PCA(whiten)+KMeans(n_init=50): Silhouette={sil_a:.4f}\")\n",
    "\n",
    "# Strategy B: L2-normalize -> KMeans(n_init=50)  (cosine近似)\n",
    "Xb = latent_vectors / (np.linalg.norm(latent_vectors, axis=1, keepdims=True) + 1e-8)\n",
    "labels_b = _KMeans(n_clusters=n_clusters, n_init=50, random_state=42).fit_predict(Xb)\n",
    "sil_b = silhouette_score(latent_vectors, labels_b)\n",
    "print(f\"[B] L2-norm+KMeans(n_init=50): Silhouette={sil_b:.4f}\")\n",
    "\n",
    "# Strategy C: UMAP -> HDBSCAN\n",
    "try:\n",
    "    import umap\n",
    "    import hdbscan\n",
    "    U = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=10, random_state=42).fit_transform(latent_vectors)\n",
    "    labels_c = hdbscan.HDBSCAN(min_cluster_size=20).fit_predict(U)\n",
    "    # -1 はノイズ\n",
    "    valid = labels_c != -1\n",
    "    if valid.any() and len(set(labels_c[valid]))>1:\n",
    "        sil_c = silhouette_score(latent_vectors[valid], labels_c[valid])\n",
    "        print(f\"[C] UMAP+HDBSCAN: clusters={len(set(labels_c[valid]))}, noise={(~valid).sum()}, Silhouette(valid)={sil_c:.4f}\")\n",
    "    else:\n",
    "        print(f\"[C] UMAP+HDBSCAN: clusters={len(set(labels_c)) if len(set(labels_c))>1 else 0}, noise={(labels_c==-1).sum()} (silhouette N/A)\")\n",
    "except Exception as e:\n",
    "    print(f\"[C] UMAP+HDBSCAN unavailable: {e}\")\n",
    "\n",
    "# 既定の出力を[A]に設定\n",
    "cluster_labels = labels_a\n",
    "# Phase 1との比較\n",
    "phase1_score = 0.565\n",
    "improvement = ((silhouette_avg - phase1_score) / phase1_score) * 100\n",
    "\n",
    "print(f\"📈 Performance Comparison:\")\n",
    "print(f\"   Phase 1 (30 frames): {phase1_score:.4f}\")\n",
    "print(f\"   Phase 4 (last 64 frames): {silhouette_avg:.4f}\")\n",
    "print(f\"   Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "if improvement >= 15:\n",
    "print(\"🎉 Phase 4 目標達成！ (15%以上の向上) - Last 64 frames strategy successful!\")\n",
    "else:\n",
    "print(f\"⚠️  Phase 4 目標未達 ({improvement:.1f}% < 15%) - Consider further optimization\")\n",
    "\n",
    "return {\n",
    "'model': model,\n",
    "'losses': losses,\n",
    "'silhouette_score': silhouette_avg,\n",
    "'calinski_score': calinski_score,\n",
    "'davies_bouldin': davies_bouldin,\n",
    "'latent_vectors': latent_vectors,\n",
    "'cluster_labels': cluster_labels,\n",
    "'f_values': f_values,\n",
    "'k_values': k_values,\n",
    "'improvement': improvement,\n",
    "'frames_used': 'last_64'\n",
    "}\n",
    "\n",
    "# 実行\n",
    "print(\"🚀 Starting Phase 4 training with last 64 frames...\")\n",
    "results = run_phase4_training_last64()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure latent_vectors are available if only results exist\n",
    "if 'latent_vectors' not in globals():\n",
    "    if 'results' in globals() and isinstance(results, dict) and 'latent_vectors' in results:\n",
    "        latent_vectors = results['latent_vectors']\n",
    "        f_values = results.get('f_values', None)\n",
    "        k_values = results.get('k_values', None)\n",
    "        cluster_labels = results.get('cluster_labels', None)\n",
    "        print('Recovered latent_vectors from results dict:', latent_vectors.shape)\n",
    "    else:\n",
    "        print('latent_vectors not found. Run training to produce results and latent vectors.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Step 4: 結果保存・ダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果保存とダウンロード（Last 64 frames版）\n",
    "import pickle\n",
    "from google.colab import files\n",
    "\n",
    "# 結果をGoogle Driveに保存\n",
    "results_path = '/content/drive/MyDrive/GrayScottML/phase4_results_last64_gpu.pkl'\n",
    "model_path = '/content/drive/MyDrive/GrayScottML/phase4_model_last64_gpu.pth'\n",
    "\n",
    "# 結果保存\n",
    "with open(results_path, 'wb') as f:\n",
    "    # モデルは除いて保存（サイズ削減）\n",
    "    save_results = {k: v for k, v in results.items() if k != 'model'}\n",
    "    pickle.dump(save_results, f)\n",
    "\n",
    "# モデル保存\n",
    "torch.save(results['model'].state_dict(), model_path)\n",
    "\n",
    "print(f\"💾 Results saved to Google Drive:\")\n",
    "print(f\"   📊 Results: {results_path}\")\n",
    "print(f\"   🧠 Model: {model_path}\")\n",
    "\n",
    "# ローカルにもダウンロード用ファイル作成\n",
    "local_results_path = 'phase4_results_last64_gpu.pkl'\n",
    "local_model_path = 'phase4_model_last64_gpu.pth'\n",
    "\n",
    "with open(local_results_path, 'wb') as f:\n",
    "    save_results = {k: v for k, v in results.items() if k != 'model'}\n",
    "    pickle.dump(save_results, f)\n",
    "\n",
    "torch.save(results['model'].state_dict(), local_model_path)\n",
    "\n",
    "print(\"\\n📥 Downloading files...\")\n",
    "files.download(local_results_path)\n",
    "files.download(local_model_path)\n",
    "\n",
    "print(\"✅ Download completed!\")\n",
    "print(f\"🎯 Final Results (Last 64 Frames):\")\n",
    "print(f\"   ⭐ Silhouette Score: {results['silhouette_score']:.4f}\")\n",
    "print(f\"   📈 Improvement: {results['improvement']:+.1f}%\")\n",
    "print(f\"   🎬 Strategy: Using last 64 frames for more stable patterns\")\n",
    "print(f\"   🏆 Target: {'✅ ACHIEVED' if results['improvement'] >= 15 else '❌ NOT ACHIEVED'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 5: 可視化・分析（Optional）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Google Drive save directory for figures\n",
    "import os\n",
    "SAVE_DIR = os.environ.get('GSML_SAVE_DIR', '/content/drive/MyDrive/GrayScottML/results')\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print('Figures will be saved to:', SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の詳細可視化（Last 64 frames）\n",
    "def visualize_results_last64(results):\n",
    "    latent_vectors = results['latent_vectors']\n",
    "    cluster_labels = results['cluster_labels']\n",
    "    f_values = results['f_values']\n",
    "    k_values = results['k_values']\n",
    "    \n",
    "    # PCAとt-SNEによる次元削減\n",
    "    print(\"🔍 Performing dimensionality reduction...\")\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_result = pca.fit_transform(latent_vectors)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    tsne_result = tsne.fit_transform(latent_vectors)\n",
    "    \n",
    "    # 可視化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Phase 2 Results: Phase 4 (Last 64 Frames) Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # f-k空間でのクラスタリング結果\n",
    "    scatter1 = axes[0, 0].scatter(f_values, k_values, c=cluster_labels, cmap='tab10', alpha=0.7, s=20)\n",
    "    axes[0, 0].set_xlabel('f parameter')\n",
    "    axes[0, 0].set_ylabel('k parameter')\n",
    "    axes[0, 0].set_title('Clustering Results in f-k Parameter Space')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster ID')\n",
    "    \n",
    "    # PCA結果\n",
    "    scatter2 = axes[0, 1].scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7, s=20)\n",
    "    axes[0, 1].set_title(f'PCA of Latent Space (Phase 4 (Last 64 Frames))')\n",
    "    axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.colorbar(scatter2, ax=axes[0, 1], label='Cluster ID')\n",
    "    \n",
    "    # t-SNE結果\n",
    "    scatter3 = axes[1, 0].scatter(tsne_result[:, 0], tsne_result[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7, s=20)\n",
    "    axes[1, 0].set_title('t-SNE of Latent Space (Phase 4 (Last 64 Frames))')\n",
    "    plt.colorbar(scatter3, ax=axes[1, 0], label='Cluster ID')\n",
    "    \n",
    "    # クラスター統計\n",
    "    axes[1, 1].axis('off')\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    stats_text = f\"Phase 4 (Last 64 Frames) Analysis:\\\\n\\\\n\"\n",
    "    stats_text += f\"Silhouette Score: {results['silhouette_score']:.4f}\\\\n\"\n",
    "    stats_text += f\"Improvement: {results['improvement']:+.1f}%\\\\n\\\\n\"\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        mask = cluster_labels == i\n",
    "        count = np.sum(mask)\n",
    "        f_mean = f_values[mask].mean()\n",
    "        k_mean = k_values[mask].mean()\n",
    "        stats_text += f\"Cluster {i}: {count} samples\\\\n\"\n",
    "        stats_text += f\"  f_avg: {f_mean:.4f}\\\\n\"\n",
    "        stats_text += f\"  k_avg: {k_mean:.4f}\\\\n\\\\n\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'phase4_clustering_overview.png'), dpi=200, bbox_inches='tight'); plt.close()\n",
    "    \n",
    "    print(\"📊 Visualization completed!\")\n",
    "    print(f\"🎬 Strategy: Last 64 frames captured more stable, mature patterns\")\n",
    "    print(f\"⭐ Final Score: {results['silhouette_score']:.4f}\")\n",
    "\n",
    "# 可視化実行\n",
    "print(\"🎨 Creating visualizations...\")\n",
    "visualize_results_last64(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
