#!/usr/bin/env python3
"""
1500ã‚µãƒ³ãƒ—ãƒ«å¯¾å¿œ å¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
latent_representations_frames_all.pkl ã‹ã‚‰ç›´æ¥å¯è¦–åŒ–
"""

import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import matplotlib as mpl

def load_new_data():
    """1500ã‚µãƒ³ãƒ—ãƒ«ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    
    print("ğŸ“‚ 1500ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    data_file = '../results/latent_representations_frames_all.pkl'
    
    if not os.path.exists(data_file):
        print(f"âŒ {data_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    with open(data_file, 'rb') as f:
        data = pickle.load(f)
    
    latent_vectors = data['latent_vectors']  # (1500, 128)
    filenames = data['filenames']            # 1500å€‹
    f_values = data['f_values']              # 1500å€‹
    k_values = data['k_values']              # 1500å€‹
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(filenames)}")
    print(f"   æ½œåœ¨æ¬¡å…ƒ: {latent_vectors.shape[1]}")
    print(f"   få€¤ç¯„å›²: {f_values.min():.4f} - {f_values.max():.4f}")
    print(f"   kå€¤ç¯„å›²: {k_values.min():.4f} - {k_values.max():.4f}")
    
    return {
        'latent_vectors': latent_vectors,
        'filenames': filenames,
        'f_values': f_values,
        'k_values': k_values
    }

def perform_clustering(latent_vectors, n_clusters=20):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    
    print(f"\nğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­ (k={n_clusters})...")
    
    # K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(latent_vectors)
    
    # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—
    silhouette = silhouette_score(latent_vectors, cluster_labels)
    
    print(f"âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
    print(f"   ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {silhouette:.3f}")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒ
    unique, counts = np.unique(cluster_labels, return_counts=True)
    print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†å¸ƒ:")
    for cluster, count in zip(unique, counts):
        print(f"     Cluster {cluster}: {count} samples ({count/len(cluster_labels)*100:.1f}%)")
    
    return cluster_labels, silhouette

def perform_dimensionality_reduction(latent_vectors):
    """æ¬¡å…ƒå‰Šæ¸›ã‚’å®Ÿè¡Œ"""
    
    print(f"\nğŸ“‰ æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œä¸­...")
    
    # PCA
    print("   PCAå®Ÿè¡Œä¸­...")
    pca = PCA(n_components=2, random_state=42)
    pca_result = pca.fit_transform(latent_vectors)
    print(f"     å¯„ä¸ç‡: PC1={pca.explained_variance_ratio_[0]:.3f}, PC2={pca.explained_variance_ratio_[1]:.3f}")
    
    # t-SNE
    print("   t-SNEå®Ÿè¡Œä¸­...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    tsne_result = tsne.fit_transform(latent_vectors)
    
    print("âœ… æ¬¡å…ƒå‰Šæ¸›å®Œäº†")
    
    return pca_result, tsne_result

def create_visualizations(data, cluster_labels, pca_result, tsne_result):
    """å¯è¦–åŒ–ã‚’ä½œæˆ"""
    
    print(f"\nğŸ¨ å¯è¦–åŒ–ä½œæˆä¸­...")
    
    f_values = data['f_values']
    k_values = data['k_values']
    
    # å®‰å…¨ãªã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’é¸æŠ
    try:
        colormap = 'viridis'
        test_cmap = plt.cm.get_cmap(colormap)
        print(f"âœ… Using colormap: {colormap}")
    except:
        colormap = 'jet'
        print(f"âš ï¸  viridis not available, using fallback colormap: {colormap}")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¥ã®è‰²ã‚’ç”Ÿæˆ
    unique_clusters = np.unique(cluster_labels)
    colors = plt.cm.get_cmap(colormap)(np.linspace(0, 1, len(unique_clusters)))
    
    # 1. f-kç©ºé–“ã§ã®å¯è¦–åŒ–
    plt.figure(figsize=(12, 8))
    
    for i, cluster in enumerate(unique_clusters):
        mask = cluster_labels == cluster
        count = np.sum(mask)
        plt.scatter(f_values[mask], k_values[mask], 
                   c=[colors[i]], alpha=0.7, s=30, 
                   label=f'Cluster {int(cluster)} ({count})')
    
    plt.xlabel('f parameter', fontsize=12)
    plt.ylabel('k parameter', fontsize=12)
    plt.title('Gray-Scott Clustering Results (1500 samples)\nf-k Parameter Space', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, alpha=0.3)
    plt.gca().invert_yaxis()
    plt.tight_layout()
    
    # ä¿å­˜
    output_file = '../results/fk_scatter_1500samples.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… Saved: {output_file}")
    
    # 2. PCAå¯è¦–åŒ–
    plt.figure(figsize=(10, 8))
    
    for i, cluster in enumerate(unique_clusters):
        mask = cluster_labels == cluster
        count = np.sum(mask)
        plt.scatter(pca_result[mask, 0], pca_result[mask, 1], 
                   c=[colors[i]], alpha=0.7, s=30, 
                   label=f'Cluster {int(cluster)} ({count})')
    
    plt.xlabel('PCA Component 1', fontsize=12)
    plt.ylabel('PCA Component 2', fontsize=12)
    plt.title('PCA Visualization (1500 samples)\nLatent Space', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # ä¿å­˜
    output_file = '../results/pca_scatter_1500samples.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… Saved: {output_file}")
    
    # 3. t-SNEå¯è¦–åŒ–
    plt.figure(figsize=(10, 8))
    
    for i, cluster in enumerate(unique_clusters):
        mask = cluster_labels == cluster
        count = np.sum(mask)
        plt.scatter(tsne_result[mask, 0], tsne_result[mask, 1], 
                   c=[colors[i]], alpha=0.7, s=30, 
                   label=f'Cluster {int(cluster)} ({count})')
    
    plt.xlabel('t-SNE Component 1', fontsize=12)
    plt.ylabel('t-SNE Component 2', fontsize=12)
    plt.title('t-SNE Visualization (1500 samples)\nLatent Space', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # ä¿å­˜
    output_file = '../results/tsne_scatter_1500samples.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… Saved: {output_file}")

def print_cluster_statistics(data, cluster_labels):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆã‚’è¡¨ç¤º"""
    
    print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆ (1500ã‚µãƒ³ãƒ—ãƒ«)")
    print("-" * 60)
    
    f_values = data['f_values']
    k_values = data['k_values']
    unique_clusters = np.unique(cluster_labels)
    
    for cluster in unique_clusters:
        mask = cluster_labels == cluster
        count = np.sum(mask)
        f_mean = f_values[mask].mean()
        k_mean = k_values[mask].mean()
        f_std = f_values[mask].std()
        k_std = k_values[mask].std()
        
        print(f"Cluster {int(cluster):2d}: {count:4d} samples ({count/len(cluster_labels)*100:5.1f}%)")
        print(f"    f: {f_mean:.4f} Â± {f_std:.4f}")
        print(f"    k: {k_mean:.4f} Â± {k_std:.4f}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ¨ Gray-Scott 1500ã‚µãƒ³ãƒ—ãƒ«å¯è¦–åŒ–")
    print("=" * 50)
    print(f"ğŸ”§ Matplotlib version: {mpl.__version__}")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = load_new_data()
    if data is None:
        return
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    cluster_labels, silhouette = perform_clustering(data['latent_vectors'])
    
    # æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œ
    pca_result, tsne_result = perform_dimensionality_reduction(data['latent_vectors'])
    
    # å¯è¦–åŒ–ä½œæˆ
    create_visualizations(data, cluster_labels, pca_result, tsne_result)
    
    # çµ±è¨ˆè¡¨ç¤º
    print_cluster_statistics(data, cluster_labels)
    
    print(f"\nğŸ‰ 1500ã‚µãƒ³ãƒ—ãƒ«å¯è¦–åŒ–å®Œäº†!")
    print(f"ğŸ“ˆ æœ€çµ‚ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {silhouette:.3f}")
    print(f"ğŸ“ ä¿å­˜ã•ã‚ŒãŸç”»åƒ:")
    print(f"   - fk_scatter_1500samples.png")
    print(f"   - pca_scatter_1500samples.png") 
    print(f"   - tsne_scatter_1500samples.png")

if __name__ == "__main__":
    main() 